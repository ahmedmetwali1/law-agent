"""
Advanced Intelligent Prompts System v2.0
Ù†Ø¸Ø§Ù… Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨ØªØ§Øª Ø§Ù„Ø°ÙƒÙŠØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…

Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª:
1. Semantic Domain Detection
2. Fuzzy Keyword Matching
3. Dynamic Keyword Expansion
4. Few-Shot Learning Templates
5. Prompt Versioning & A/B Testing
6. User Preference Learning
7. Context-Aware Magic Words
8. Prompt Chaining
9. Performance Tracking
10. Adaptive Complexity Assessment
"""

import re
import logging
import hashlib
import json
from typing import List, Dict, Any, Optional, Tuple, Set, Callable
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime
from collections import defaultdict
import numpy as np
from difflib import SequenceMatcher
from abc import ABC, abstractmethod

logger = logging.getLogger(__name__)


# ============================================================
# ENUMS - ØªØ¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø­Ø³Ù‘Ù†Ø©
# ============================================================

class LegalDomain(Enum):
    """
    Ø§Ù„Ù…Ø¬Ø§Ù„Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©
    Ù…Ø¹ metadata Ø¥Ø¶Ø§ÙÙŠØ© Ù„ÙƒÙ„ Ù…Ø¬Ø§Ù„
    """
    PERSONAL_STATUS = ("Ø£Ø­ÙˆØ§Ù„ Ø´Ø®ØµÙŠØ©", "family", ["Ø­Ø¶Ø§Ù†Ø©", "Ø·Ù„Ø§Ù‚", "Ù†ÙÙ‚Ø©", "Ø²ÙˆØ§Ø¬"])
    CIVIL = ("Ù…Ø¯Ù†ÙŠ", "civil", ["Ø¹Ù‚Ø¯", "ØªØ¹ÙˆÙŠØ¶", "Ù…Ù„ÙƒÙŠØ©"])
    CRIMINAL = ("Ø¬Ù†Ø§Ø¦ÙŠ", "criminal", ["Ø¬Ø±ÙŠÙ…Ø©", "Ø¹Ù‚ÙˆØ¨Ø©", "Ù…ØªÙ‡Ù…"])
    COMMERCIAL = ("ØªØ¬Ø§Ø±ÙŠ", "commercial", ["Ø´Ø±ÙƒØ©", "ØªØ¬Ø§Ø±Ø©", "Ø´ÙŠÙƒ"])
    LABOR = ("Ø¹Ù…Ø§Ù„ÙŠ", "labor", ["Ø¹Ø§Ù…Ù„", "ÙØµÙ„", "Ø±Ø§ØªØ¨"])
    ADMINISTRATIVE = ("Ø¥Ø¯Ø§Ø±ÙŠ", "administrative", ["Ù‚Ø±Ø§Ø±", "ØªØ¸Ù„Ù…", "Ø­ÙƒÙˆÙ…Ø©"])
    REAL_ESTATE = ("Ø¹Ù‚Ø§Ø±ÙŠ", "real_estate", ["Ø¹Ù‚Ø§Ø±", "ØµÙƒ", "Ø¥ÙØ±Ø§Øº"])
    WAQF_INHERITANCE = ("ÙˆÙ‚Ù ÙˆØªØ±ÙƒØ§Øª", "inheritance", ["Ù…ÙŠØ±Ø§Ø«", "ÙˆØµÙŠØ©", "ØªØ±ÙƒØ©"])
    EXECUTION = ("ØªÙ†ÙÙŠØ° ÙˆØ¥Ø¬Ø±Ø§Ø¡Ø§Øª", "execution", ["ØªÙ†ÙÙŠØ°", "Ø­Ø¬Ø²", "Ø³Ù†Ø¯"])
    GENERAL_REGULATIONS = ("Ø£Ù†Ø¸Ù…Ø© Ø¹Ø§Ù…Ø©", "general", ["Ù†Ø¸Ø§Ù…", "Ù„Ø§Ø¦Ø­Ø©"])
    UNKNOWN = ("ØºÙŠØ± Ù…Ø­Ø¯Ø¯", "unknown", [])
    
    def __init__(self, arabic_name: str, english_key: str, core_keywords: List[str]):
        self.arabic_name = arabic_name
        self.english_key = english_key
        self.core_keywords = core_keywords
    
    @property
    def db_value(self) -> str:
        return self.arabic_name
    
    @classmethod
    def from_string(cls, value: str) -> 'LegalDomain':
        """ØªØ­ÙˆÙŠÙ„ Ù…Ù† Ù†Øµ Ø¥Ù„Ù‰ enum"""
        for domain in cls:
            if domain.arabic_name == value or domain.english_key == value:
                return domain
        return cls.UNKNOWN


class QuestionComplexity(Enum):
    """Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ Ù…Ø¹ Ø£ÙˆØ²Ø§Ù†"""
    SIMPLE = ("simple", 1, "Ø³Ø¤Ø§Ù„ Ù…Ø¨Ø§Ø´Ø±")
    MODERATE = ("moderate", 2, "ÙŠØ­ØªØ§Ø¬ ØªØ­Ù„ÙŠÙ„")
    COMPLEX = ("complex", 3, "Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯")
    EXPERT = ("expert", 4, "ÙŠØ­ØªØ§Ø¬ Ø®Ø¨Ø±Ø© Ø¹Ù…ÙŠÙ‚Ø©")  # Ø¬Ø¯ÙŠØ¯
    
    def __init__(self, key: str, level: int, description: str):
        self.key = key
        self.level = level
        self.description = description


class PromptStyle(Enum):
    """Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª"""
    FORMAL = "formal"           # Ø±Ø³Ù…ÙŠ Ù‚Ø§Ù†ÙˆÙ†ÙŠ
    CONVERSATIONAL = "conversational"  # Ù…Ø­Ø§Ø¯Ø«Ø©
    ACADEMIC = "academic"       # Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ
    PRACTICAL = "practical"     # Ø¹Ù…Ù„ÙŠ ØªØ·Ø¨ÙŠÙ‚ÙŠ


class MagicWordCategory(Enum):
    """ÙØ¦Ø§Øª Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø³Ø­Ø±ÙŠØ©"""
    STEP_BY_STEP = "step_by_step"
    DEEP_THINKING = "deep_thinking"
    EXPERT_ROLE = "expert_role"
    VERIFICATION = "verification"
    COUNTER_ARGUMENT = "counter_argument"
    STRUCTURED_OUTPUT = "structured_output"
    CONFIDENCE = "confidence"       # Ø¬Ø¯ÙŠØ¯
    EXAMPLES = "examples"           # Ø¬Ø¯ÙŠØ¯
    LIMITATIONS = "limitations"     # Ø¬Ø¯ÙŠØ¯


# ============================================================
# DATA STRUCTURES - Ù‡ÙŠØ§ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©
# ============================================================

@dataclass
class KeywordMatch:
    """Ù…Ø·Ø§Ø¨Ù‚Ø© ÙƒÙ„Ù…Ø© Ù…ÙØªØ§Ø­ÙŠØ©"""
    keyword: str
    matched_text: str
    similarity: float
    domain: LegalDomain
    is_exact: bool


@dataclass
class DomainScore:
    """Ù†ØªÙŠØ¬Ø© ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¬Ø§Ù„"""
    domain: LegalDomain
    score: float
    matched_keywords: List[str]
    confidence: float
    reasoning: str


@dataclass
class AnalyzedQuestion:
    """Ù†ØªÙŠØ¬Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©"""
    original: str
    normalized: str  # Ø¬Ø¯ÙŠØ¯: Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù†Ø¸Ù
    keywords: List[str]
    legal_keywords: List[KeywordMatch]  # Ù…Ø­Ø³Ù‘Ù†
    domain: LegalDomain
    domain_scores: List[DomainScore]  # Ø¬Ø¯ÙŠØ¯: ÙƒÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    secondary_domains: List[LegalDomain]  # Ø¬Ø¯ÙŠØ¯
    complexity: QuestionComplexity
    complexity_factors: Dict[str, float]  # Ø¬Ø¯ÙŠØ¯
    sub_questions: List[str]
    search_queries: List[str]
    intent: str  # Ø¬Ø¯ÙŠØ¯: Ù†ÙŠØ© Ø§Ù„Ø³Ø¤Ø§Ù„
    entities: Dict[str, List[str]]  # Ø¬Ø¯ÙŠØ¯: Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
    timestamp: datetime = field(default_factory=datetime.now)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "original": self.original,
            "domain": self.domain.arabic_name,
            "complexity": self.complexity.key,
            "keywords": self.keywords,
            "legal_keywords": [k.keyword for k in self.legal_keywords],
            "sub_questions": self.sub_questions,
            "search_queries": self.search_queries,
            "intent": self.intent
        }


@dataclass
class PromptTemplate:
    """Ù‚Ø§Ù„Ø¨ Ø¨Ø±ÙˆÙ…Ø¨Øª Ù…Ø­Ø³Ù‘Ù†"""
    template_id: str
    name: str
    version: str
    template: str
    variables: List[str]
    category: str
    style: PromptStyle
    performance_score: float = 0.0
    usage_count: int = 0
    created_at: datetime = field(default_factory=datetime.now)
    
    def render(self, **kwargs) -> str:
        """ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ù‚Ø§Ù„Ø¨ Ø¨Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª"""
        result = self.template
        for var in self.variables:
            if var in kwargs:
                result = result.replace(f"{{{var}}}", str(kwargs[var]))
        return result


@dataclass
class FewShotExample:
    """Ù…Ø«Ø§Ù„ Ù„Ù„ØªØ¹Ù„Ù…"""
    question: str
    answer: str
    domain: LegalDomain
    quality_score: float
    source: str


@dataclass
class PromptPerformance:
    """Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª"""
    template_id: str
    success_rate: float
    avg_response_quality: float
    avg_response_time: float
    usage_count: int
    last_used: datetime


# ============================================================
# MAGIC WORDS - Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø³Ø­Ø±ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©
# ============================================================

class MagicWordsManager:
    """Ù…Ø¯ÙŠØ± Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø³Ø­Ø±ÙŠØ©"""
    
    def __init__(self):
        self.words = self._initialize_words()
        self.context_rules = self._initialize_context_rules()
        self.performance = defaultdict(lambda: {"uses": 0, "success": 0})
    
    def _initialize_words(self) -> Dict[MagicWordCategory, List[Dict[str, Any]]]:
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø³Ø­Ø±ÙŠØ© Ù…Ø¹ metadata"""
        return {
            MagicWordCategory.STEP_BY_STEP: [
                {"text": "ÙÙƒÙ‘Ø± Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©", "weight": 1.0, "lang": "ar"},
                {"text": "Let's think step by step", "weight": 0.9, "lang": "en"},
                {"text": "Ø­Ù„Ù„ Ù‡Ø°Ø§ Ø¨Ø´ÙƒÙ„ Ù…Ù†Ù‡Ø¬ÙŠ ÙˆÙ…ØªØ³Ù„Ø³Ù„", "weight": 0.95, "lang": "ar"},
                {"text": "Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©ØŒ Ø±ØªÙ‘Ø¨ Ø£ÙÙƒØ§Ø±Ùƒ Ø¨ØªØ³Ù„Ø³Ù„ Ù…Ù†Ø·Ù‚ÙŠ", "weight": 0.85, "lang": "ar"},
                {"text": "Ø§Ø¨Ø¯Ø£ Ø¨Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ§Øª Ø«Ù… Ø§Ù†ØªÙ‚Ù„ Ù„Ù„ØªÙØ§ØµÙŠÙ„", "weight": 0.8, "lang": "ar"}
            ],
            MagicWordCategory.DEEP_THINKING: [
                {"text": "ÙÙƒÙ‘Ø± Ø¨Ø¹Ù…Ù‚ ÙˆØªØ£Ù†ÙÙ‘ ÙÙŠ ÙƒÙ„ Ø¬Ø§Ù†Ø¨", "weight": 1.0, "lang": "ar"},
                {"text": "Ø®Ø° ÙˆÙ‚ØªÙƒ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„", "weight": 0.9, "lang": "ar"},
                {"text": "Ù„Ø§ ØªØªØ³Ø±Ø¹ØŒ Ø§Ù„Ø¯Ù‚Ø© Ø£Ù‡Ù… Ù…Ù† Ø§Ù„Ø³Ø±Ø¹Ø©", "weight": 0.85, "lang": "ar"},
                {"text": "ØªØ£Ù…Ù„ ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª", "weight": 0.8, "lang": "ar"}
            ],
            MagicWordCategory.EXPERT_ROLE: [
                {"text": "Ø£Ù†Øª Ù…Ø­Ø§Ù…Ù Ø³Ø¹ÙˆØ¯ÙŠ Ø®Ø¨ÙŠØ± Ø¨Ø®Ø¨Ø±Ø© 20 Ø¹Ø§Ù…Ø§Ù‹ ÙÙŠ {domain}", "weight": 1.0, "lang": "ar", "vars": ["domain"]},
                {"text": "ØªØµØ±Ù‘Ù ÙƒÙ‚Ø§Ø¶Ù ÙÙŠ Ø§Ù„Ù…Ø­ÙƒÙ…Ø© Ø§Ù„Ø¹Ù„ÙŠØ§ ÙŠÙØ­Øµ Ø§Ù„Ù‚Ø¶ÙŠØ©", "weight": 0.95, "lang": "ar"},
                {"text": "Ø£Ù†Øª Ù…Ø³ØªØ´Ø§Ø± Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø£ÙˆÙ„ ÙÙŠ Ù…ÙƒØªØ¨ Ù…Ø­Ø§Ù…Ø§Ø© Ø±Ø§Ø¦Ø¯", "weight": 0.9, "lang": "ar"},
                {"text": "Ø¨ØµÙØªÙƒ Ø®Ø¨ÙŠØ±Ø§Ù‹ ÙÙŠ {domain}ØŒ Ù‚Ø¯Ù… ØªØ­Ù„ÙŠÙ„Ùƒ Ø§Ù„Ù…Ù‡Ù†ÙŠ", "weight": 0.85, "lang": "ar", "vars": ["domain"]}
            ],
            MagicWordCategory.VERIFICATION: [
                {"text": "ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ø³ØªÙ†ØªØ§Ø¬Ø§ØªÙƒ Ù‚Ø¨Ù„ ØªÙ‚Ø¯ÙŠÙ…Ù‡Ø§", "weight": 1.0, "lang": "ar"},
                {"text": "Ù…Ø§ Ø§Ù„Ø£Ø¯Ù„Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„ØªÙŠ ØªØ¯Ø¹Ù… Ù‡Ø°Ø§ Ø§Ù„Ø±Ø£ÙŠØŸ", "weight": 0.95, "lang": "ar"},
                {"text": "Ù‡Ù„ Ù‡Ù†Ø§Ùƒ Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª Ù„Ù‡Ø°Ù‡ Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø©ØŸ", "weight": 0.9, "lang": "ar"},
                {"text": "Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù…Ù†Ø·Ù‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰", "weight": 0.85, "lang": "ar"}
            ],
            MagicWordCategory.COUNTER_ARGUMENT: [
                {"text": "Ù…Ø§ Ø§Ù„Ø­Ø¬Ø¬ Ø§Ù„Ù…Ø¶Ø§Ø¯Ø© Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø© Ø§Ù„ØªÙŠ Ù‚Ø¯ ÙŠØ«ÙŠØ±Ù‡Ø§ Ø§Ù„Ø®ØµÙ…ØŸ", "weight": 1.0, "lang": "ar"},
                {"text": "ÙÙƒÙ‘Ø± Ù…Ù† ÙˆØ¬Ù‡Ø© Ù†Ø¸Ø± Ø§Ù„Ø·Ø±Ù Ø§Ù„Ø¢Ø®Ø±", "weight": 0.95, "lang": "ar"},
                {"text": "Ù…Ø§ Ø§Ù„Ø°ÙŠ Ù‚Ø¯ ÙŠÙØ¶Ø¹Ù Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆÙ‚Ù Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØŸ", "weight": 0.9, "lang": "ar"},
                {"text": "ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† Ù„Ù„Ø®ØµÙ… Ø£Ù† ÙŠØ·Ø¹Ù† ÙÙŠ Ù‡Ø°Ø§ØŸ", "weight": 0.85, "lang": "ar"}
            ],
            MagicWordCategory.STRUCTURED_OUTPUT: [
                {"text": "Ù†Ø¸Ù‘Ù… Ø¥Ø¬Ø§Ø¨ØªÙƒ Ø¨ÙˆØ¶ÙˆØ­ Ù…Ø¹ Ø¹Ù†Ø§ÙˆÙŠÙ† ÙˆÙÙ‚Ø±Ø§Øª", "weight": 1.0, "lang": "ar"},
                {"text": "Ø§Ø¨Ø¯Ø£ Ø¨Ø§Ù„Ø®Ù„Ø§ØµØ© Ø«Ù… Ù‚Ø¯Ù… Ø§Ù„ØªÙØ§ØµÙŠÙ„", "weight": 0.95, "lang": "ar"},
                {"text": "Ø§Ø³ØªØ®Ø¯Ù… ØªØ¹Ø¯Ø§Ø¯Ø§Ù‹ Ù†Ù‚Ø·ÙŠØ§Ù‹ Ù„Ù„ÙˆØ¶ÙˆØ­", "weight": 0.9, "lang": "ar"},
                {"text": "Ù‚Ø³Ù‘Ù… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¥Ù„Ù‰: Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø©ØŒ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ØŒ Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬", "weight": 0.85, "lang": "ar"}
            ],
            MagicWordCategory.CONFIDENCE: [
                {"text": "Ø­Ø¯Ø¯ Ù…Ø³ØªÙˆÙ‰ Ø«Ù‚ØªÙƒ ÙÙŠ ÙƒÙ„ Ø§Ø³ØªÙ†ØªØ§Ø¬ (Ø¹Ø§Ù„ÙŠ/Ù…ØªÙˆØ³Ø·/Ù…Ù†Ø®ÙØ¶)", "weight": 1.0, "lang": "ar"},
                {"text": "Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…ØªØ£ÙƒØ¯Ø§Ù‹ØŒ Ø§Ø°ÙƒØ± Ø°Ù„Ùƒ ØµØ±Ø§Ø­Ø©", "weight": 0.95, "lang": "ar"},
                {"text": "Ù…ÙŠÙ‘Ø² Ø¨ÙŠÙ† Ø§Ù„Ø­Ù‚Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø¤ÙƒØ¯Ø© ÙˆØ§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª", "weight": 0.9, "lang": "ar"}
            ],
            MagicWordCategory.EXAMPLES: [
                {"text": "Ù‚Ø¯Ù… Ø£Ù…Ø«Ù„Ø© Ø¹Ù…Ù„ÙŠØ© ØªÙˆØ¶ÙŠØ­ÙŠØ©", "weight": 1.0, "lang": "ar"},
                {"text": "Ø§Ø³ØªØ´Ù‡Ø¯ Ø¨Ø³ÙˆØ§Ø¨Ù‚ Ù‚Ø¶Ø§Ø¦ÙŠØ© Ø¥Ù† ÙˆØ¬Ø¯Øª", "weight": 0.95, "lang": "ar"},
                {"text": "ÙˆØ¶Ù‘Ø­ Ø¨Ø­Ø§Ù„Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ù…Ø´Ø§Ø¨Ù‡Ø©", "weight": 0.9, "lang": "ar"}
            ],
            MagicWordCategory.LIMITATIONS: [
                {"text": "Ø§Ø°ÙƒØ± Ø­Ø¯ÙˆØ¯ Ù‡Ø°Ø§ Ø§Ù„Ø±Ø£ÙŠ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ", "weight": 1.0, "lang": "ar"},
                {"text": "ÙˆØ¶Ù‘Ø­ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„ØªÙŠ Ù„Ø§ ÙŠÙ†Ø·Ø¨Ù‚ Ø¹Ù„ÙŠÙ‡Ø§ Ù‡Ø°Ø§", "weight": 0.95, "lang": "ar"},
                {"text": "Ù†Ø¨Ù‘Ù‡ Ø¥Ù„Ù‰ Ù…Ø§ ÙŠØ­ØªØ§Ø¬ Ø§Ø³ØªØ´Ø§Ø±Ø© Ù…ØªØ®ØµØµ", "weight": 0.9, "lang": "ar"}
            ]
        }
    
    def _initialize_context_rules(self) -> Dict[str, List[MagicWordCategory]]:
        """Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ø³ÙŠØ§Ù‚"""
        return {
            "simple": [MagicWordCategory.STRUCTURED_OUTPUT],
            "moderate": [
                MagicWordCategory.STEP_BY_STEP,
                MagicWordCategory.STRUCTURED_OUTPUT
            ],
            "complex": [
                MagicWordCategory.DEEP_THINKING,
                MagicWordCategory.STEP_BY_STEP,
                MagicWordCategory.VERIFICATION,
                MagicWordCategory.COUNTER_ARGUMENT
            ],
            "expert": [
                MagicWordCategory.DEEP_THINKING,
                MagicWordCategory.EXPERT_ROLE,
                MagicWordCategory.VERIFICATION,
                MagicWordCategory.COUNTER_ARGUMENT,
                MagicWordCategory.CONFIDENCE,
                MagicWordCategory.LIMITATIONS
            ],
            "controversial": [
                MagicWordCategory.COUNTER_ARGUMENT,
                MagicWordCategory.VERIFICATION,
                MagicWordCategory.CONFIDENCE
            ]
        }
    
    def get_words(
        self,
        categories: List[MagicWordCategory],
        domain: Optional[LegalDomain] = None,
        max_per_category: int = 2
    ) -> List[str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ø³Ø­Ø±ÙŠØ©"""
        result = []
        
        for category in categories:
            if category not in self.words:
                continue
            
            words = self.words[category]
            # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„ÙˆØ²Ù†
            sorted_words = sorted(words, key=lambda w: w["weight"], reverse=True)
            
            for word_data in sorted_words[:max_per_category]:
                text = word_data["text"]
                
                # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª
                if "vars" in word_data and domain:
                    text = text.replace("{domain}", domain.arabic_name)
                
                result.append(text)
        
        return result
    
    def get_context_words(
        self,
        complexity: QuestionComplexity,
        domain: Optional[LegalDomain] = None,
        is_controversial: bool = False
    ) -> List[str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ø³ÙŠØ§Ù‚"""
        context = complexity.key
        if is_controversial:
            context = "controversial"
        
        categories = self.context_rules.get(context, [MagicWordCategory.STEP_BY_STEP])
        return self.get_words(categories, domain)
    
    def record_usage(self, category: MagicWordCategory, success: bool):
        """ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…"""
        self.performance[category.value]["uses"] += 1
        if success:
            self.performance[category.value]["success"] += 1


# ============================================================
# FUZZY KEYWORD MATCHER - Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¶Ø¨Ø§Ø¨ÙŠØ©
# ============================================================

class FuzzyKeywordMatcher:
    """Ù…Ø·Ø§Ø¨Ù‚ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ø°ÙƒÙŠ"""
    
    def __init__(self, similarity_threshold: float = 0.7):
        self.threshold = similarity_threshold
        self.keyword_cache: Dict[str, List[str]] = {}
        self.synonyms = self._load_synonyms()
    
    def _load_synonyms(self) -> Dict[str, List[str]]:
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ØªØ±Ø§Ø¯ÙØ§Øª"""
        return {
            # Ø£Ø­ÙˆØ§Ù„ Ø´Ø®ØµÙŠØ©
            "Ø­Ø¶Ø§Ù†Ø©": ["Ø±Ø¹Ø§ÙŠØ©", "ÙƒÙØ§Ù„Ø©", "Ø­ÙØ¸"],
            "Ø·Ù„Ø§Ù‚": ["ÙØ±Ø§Ù‚", "Ø§Ù†ÙØµØ§Ù„", "ØªØ·Ù„ÙŠÙ‚"],
            "Ù†ÙÙ‚Ø©": ["Ø¥Ù†ÙØ§Ù‚", "Ù…Ø¹Ø§Ø´", "Ù…ØµØ±ÙˆÙ"],
            "Ø²ÙŠØ§Ø±Ø©": ["Ø±Ø¤ÙŠØ©", "Ù…Ø´Ø§Ù‡Ø¯Ø©", "Ù„Ù‚Ø§Ø¡"],
            # Ø¬Ù†Ø§Ø¦ÙŠ
            "Ø¬Ø±ÙŠÙ…Ø©": ["Ø¬Ù†Ø§ÙŠØ©", "Ø¬Ø±Ù…", "Ù…Ø®Ø§Ù„ÙØ©"],
            "Ø¹Ù‚ÙˆØ¨Ø©": ["Ø¬Ø²Ø§Ø¡", "Ø¹Ù‚Ø§Ø¨", "Ø­Ø¯"],
            "Ø³Ø¬Ù†": ["Ø­Ø¨Ø³", "Ø¥ÙŠØ¯Ø§Ø¹", "ØªÙˆÙ‚ÙŠÙ"],
            # Ù…Ø¯Ù†ÙŠ
            "Ø¹Ù‚Ø¯": ["Ø§ØªÙØ§Ù‚", "Ø§ØªÙØ§Ù‚ÙŠØ©", "Ø¹Ù‡Ø¯"],
            "ØªØ¹ÙˆÙŠØ¶": ["Ø¬Ø¨Ø±", "ØªØ¹ÙˆÙŠØ¶Ø§Øª", "Ø£Ø±Ø´"],
            "Ù…Ù„ÙƒÙŠØ©": ["ØªÙ…Ù„Ùƒ", "Ø§Ø³ØªØ­Ù‚Ø§Ù‚", "Ø­ÙŠØ§Ø²Ø©"],
            # Ø¹Ù…Ø§Ù„ÙŠ
            "ÙØµÙ„": ["Ø¥Ù†Ù‡Ø§Ø¡", "Ø·Ø±Ø¯", "Ø¥Ù‚Ø§Ù„Ø©"],
            "Ø±Ø§ØªØ¨": ["Ø£Ø¬Ø±", "Ù…Ø±ØªØ¨", "Ù…Ù‚Ø§Ø¨Ù„"],
            "Ø¹Ø§Ù…Ù„": ["Ù…ÙˆØ¸Ù", "Ù…Ø³ØªØ®Ø¯Ù…", "Ø¹Ù…Ø§Ù„"],
            # Ø¥Ø¶Ø§ÙØ§Øª
            "Ø¯Ø¹ÙˆÙ‰": ["Ù‚Ø¶ÙŠØ©", "Ø¯Ø¹Ø§ÙˆÙ‰", "Ø´ÙƒÙˆÙ‰"],
            "Ø­ÙƒÙ…": ["Ù‚Ø±Ø§Ø±", "Ø£Ù…Ø±", "ÙØµÙ„"],
        }
    
    def similarity(self, text1: str, text2: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† Ù†ØµÙŠÙ†"""
        return SequenceMatcher(None, text1, text2).ratio()
    
    def find_matches(
        self,
        text: str,
        keywords: List[str],
        domain: Optional[LegalDomain] = None
    ) -> List[KeywordMatch]:
        """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ø·Ø§Ø¨Ù‚Ø§Øª"""
        matches = []
        words = text.split()
        
        for keyword in keywords:
            # Ù…Ø·Ø§Ø¨Ù‚Ø© Ù…Ø¨Ø§Ø´Ø±Ø©
            if keyword in text:
                matches.append(KeywordMatch(
                    keyword=keyword,
                    matched_text=keyword,
                    similarity=1.0,
                    domain=domain or LegalDomain.UNKNOWN,
                    is_exact=True
                ))
                continue
            
            # Ù…Ø·Ø§Ø¨Ù‚Ø© Ù…Ø¹ Ø§Ù„Ù…ØªØ±Ø§Ø¯ÙØ§Øª
            synonyms = self.synonyms.get(keyword, [])
            for synonym in synonyms:
                if synonym in text:
                    matches.append(KeywordMatch(
                        keyword=keyword,
                        matched_text=synonym,
                        similarity=0.9,
                        domain=domain or LegalDomain.UNKNOWN,
                        is_exact=False
                    ))
                    break
            
            # Ù…Ø·Ø§Ø¨Ù‚Ø© Ø¶Ø¨Ø§Ø¨ÙŠØ©
            for word in words:
                sim = self.similarity(keyword, word)
                if sim >= self.threshold:
                    matches.append(KeywordMatch(
                        keyword=keyword,
                        matched_text=word,
                        similarity=sim,
                        domain=domain or LegalDomain.UNKNOWN,
                        is_exact=False
                    ))
                    break
        
        return matches
    
    def expand_keywords(self, keywords: List[str]) -> List[str]:
        """ØªÙˆØ³ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø¨Ø§Ù„Ù…ØªØ±Ø§Ø¯ÙØ§Øª"""
        expanded = set(keywords)
        
        for keyword in keywords:
            if keyword in self.synonyms:
                expanded.update(self.synonyms[keyword])
        
        return list(expanded)


# ============================================================
# SEMANTIC DOMAIN DETECTOR - ÙƒØ§Ø´Ù Ø§Ù„Ù…Ø¬Ø§Ù„ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ
# ============================================================

class SemanticDomainDetector:
    """ÙƒØ§Ø´Ù Ø§Ù„Ù…Ø¬Ø§Ù„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ"""
    
    def __init__(self):
        self.fuzzy_matcher = FuzzyKeywordMatcher()
        self.domain_keywords = self._initialize_domain_keywords()
        self.domain_patterns = self._initialize_patterns()
        self.domain_weights = self._initialize_weights()
    
    def _initialize_domain_keywords(self) -> Dict[LegalDomain, List[str]]:
        """ÙƒÙ„Ù…Ø§Øª ÙƒÙ„ Ù…Ø¬Ø§Ù„"""
        return {
            LegalDomain.PERSONAL_STATUS: [
                # Ø§Ù„Ø­Ø¶Ø§Ù†Ø© ÙˆØ§Ù„Ø²ÙŠØ§Ø±Ø©
                "Ø­Ø¶Ø§Ù†Ø©", "Ø²ÙŠØ§Ø±Ø©", "Ø·Ù„Ø§Ù‚", "Ù†ÙÙ‚Ø©", "Ù…Ù‡Ø±", "Ø¹Ø¯Ø©", "Ø®Ù„Ø¹",
                "Ø£Ø¨", "Ø£Ù…", "Ø·ÙÙ„", "Ø£Ø·ÙØ§Ù„", "ÙˆÙ„ÙŠ", "ÙˆÙ„Ø§ÙŠØ©", "ÙˆØµØ§ÙŠØ©",
                "Ù…Ø­ÙƒÙ…Ø© Ø§Ù„Ø£Ø­ÙˆØ§Ù„ Ø§Ù„Ø´Ø®ØµÙŠØ©", "ØµÙƒ Ø§Ù„Ø·Ù„Ø§Ù‚", "Ø¹Ù‚Ø¯ Ø§Ù„Ø²ÙˆØ§Ø¬",
                "Ø­Ù‚ Ø§Ù„Ø±Ø¤ÙŠØ©", "Ø³ÙØ± Ø§Ù„Ø£Ø·ÙØ§Ù„", "Ø§Ù†ØªÙ‚Ø§Ù„ Ø§Ù„Ø­Ø¶Ø§Ù†Ø©",
                "Ø¥Ø³Ù‚Ø§Ø· Ø§Ù„Ø­Ø¶Ø§Ù†Ø©", "Ø³Ù† Ø§Ù„Ø­Ø¶Ø§Ù†Ø©", "Ù…ØµÙ„Ø­Ø© Ø§Ù„Ø·ÙÙ„",
                "Ù†ÙÙ‚Ø© Ø§Ù„Ø£ÙˆÙ„Ø§Ø¯", "Ù†ÙÙ‚Ø© Ø§Ù„Ø²ÙˆØ¬Ø©", "Ù†ÙÙ‚Ø© Ø§Ù„Ù…Ø·Ù„Ù‚Ø©",
                "Ø§Ù„Ø³ÙƒÙ†", "Ø§Ù„Ù…Ø³ÙƒÙ†", "Ø§Ù„Ù…Ù„Ø§Ø¨Ø³", "Ø§Ù„ØªØ¹Ù„ÙŠÙ…", "Ø§Ù„Ø¹Ù„Ø§Ø¬",
                "Ø²ÙˆØ¬", "Ø²ÙˆØ¬Ø©", "Ø¥Ø±Ø¶Ø§Ø¹", "Ø­Ø§Ø¶Ù†Ø©", "Ù…Ø­Ø¶ÙˆÙ†"
            ],
            LegalDomain.CRIMINAL: [
                "Ø¬Ø±ÙŠÙ…Ø©", "Ø¬Ù†Ø§ÙŠØ©", "Ø¬Ù†Ø­Ø©", "Ù…Ø®Ø§Ù„ÙØ©", "Ø¹Ù‚ÙˆØ¨Ø©", "Ø³Ø¬Ù†",
                "Ø­Ø¨Ø³", "ØºØ±Ø§Ù…Ø©", "Ø¥Ø¹Ø¯Ø§Ù…", "Ù‚ØµØ§Øµ", "Ø­Ø¯", "ØªØ¹Ø²ÙŠØ±",
                "Ù…ØªÙ‡Ù…", "Ù…Ø¯Ø¹Ù‰ Ø¹Ù„ÙŠÙ‡", "Ù†ÙŠØ§Ø¨Ø©", "Ø§Ø¯Ø¹Ø§Ø¡ Ø¹Ø§Ù…",
                "Ù…Ø­ÙƒÙ…Ø© Ø¬Ø²Ø§Ø¦ÙŠØ©", "Ø§Ø³ØªØ¦Ù†Ø§Ù Ø¬Ø²Ø§Ø¦ÙŠ",
                "Ø³Ø±Ù‚Ø©", "Ù‚ØªÙ„", "Ø§Ø¹ØªØ¯Ø§Ø¡", "ØªØ­Ø±Ø´", "Ù…Ø®Ø¯Ø±Ø§Øª", "Ø±Ø´ÙˆØ©",
                "Ø§Ø­ØªÙŠØ§Ù„", "ØªØ²ÙˆÙŠØ±", "Ø§Ø¨ØªØ²Ø§Ø²", "Ø¥Ø±Ù‡Ø§Ø¨", "ØºØ³ÙŠÙ„ Ø£Ù…ÙˆØ§Ù„"
            ],
            LegalDomain.CIVIL: [
                "Ø¹Ù‚Ø¯", "Ø¨ÙŠØ¹", "Ø´Ø±Ø§Ø¡", "Ø¥ÙŠØ¬Ø§Ø±", "Ø±Ù‡Ù†", "ÙƒÙØ§Ù„Ø©",
                "Ù…Ù„ÙƒÙŠØ©", "Ø­ÙŠØ§Ø²Ø©", "ØªØ¹ÙˆÙŠØ¶", "Ø¶Ø±Ø±", "Ù…Ø³Ø¤ÙˆÙ„ÙŠØ©",
                "Ø¯ÙŠÙ†", "Ø¯Ø§Ø¦Ù†", "Ù…Ø¯ÙŠÙ†", "ÙˆÙØ§Ø¡", "Ù…Ù‚Ø§ØµØ©",
                "Ù…Ø­ÙƒÙ…Ø© Ø¹Ø§Ù…Ø©", "Ø¯Ø¹ÙˆÙ‰ Ù…Ø¯Ù†ÙŠØ©", "Ø¥Ø®Ù„Ø§Ø¡", "ÙØ³Ø®",
                "Ø¶Ù…Ø§Ù†", "Ø¹ÙŠØ¨", "ØºØ¨Ù†", "ØªØ¯Ù„ÙŠØ³", "Ø¥ÙƒØ±Ø§Ù‡"
            ],
            LegalDomain.COMMERCIAL: [
                "Ø´Ø±ÙƒØ©", "ØªØ¬Ø§Ø±Ø©", "ØªØ§Ø¬Ø±", "Ø³Ø¬Ù„ ØªØ¬Ø§Ø±ÙŠ", "Ø¥ÙÙ„Ø§Ø³",
                "Ø´ÙŠÙƒ", "ÙƒÙ…Ø¨ÙŠØ§Ù„Ø©", "Ø³Ù†Ø¯", "ÙˆÙƒØ§Ù„Ø© ØªØ¬Ø§Ø±ÙŠØ©",
                "Ø¹Ù„Ø§Ù…Ø© ØªØ¬Ø§Ø±ÙŠØ©", "Ù…Ù†Ø§ÙØ³Ø©", "Ø§Ø­ØªÙƒØ§Ø±",
                "Ø´Ø±Ø§ÙƒØ©", "Ø£Ø³Ù‡Ù…", "Ø­ØµØµ", "ØªØµÙÙŠØ©", "Ø§Ù†Ø¯Ù…Ø§Ø¬"
            ],
            LegalDomain.LABOR: [
                "Ø¹Ø§Ù…Ù„", "Ù…ÙˆØ¸Ù", "ØµØ§Ø­Ø¨ Ø¹Ù…Ù„", "Ø¹Ù‚Ø¯ Ø¹Ù…Ù„", "Ø±Ø§ØªØ¨",
                "Ø¥Ø¬Ø§Ø²Ø©", "ÙØµÙ„", "Ø§Ø³ØªÙ‚Ø§Ù„Ø©", "Ù…ÙƒØ§ÙØ£Ø© Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø®Ø¯Ù…Ø©",
                "ØªØ£Ù…ÙŠÙ†Ø§Øª", "Ø³Ø§Ø¹Ø§Øª Ø¹Ù…Ù„", "Ø¥ØµØ§Ø¨Ø© Ø¹Ù…Ù„",
                "Ù†Ù‚Ù„", "ØªØ±Ù‚ÙŠØ©", "Ø¥Ù†Ø°Ø§Ø±", "Ø®ØµÙ…", "Ø¨Ø¯Ù„Ø§Øª"
            ],
            LegalDomain.ADMINISTRATIVE: [
                "Ø¬Ù‡Ø© Ø­ÙƒÙˆÙ…ÙŠØ©", "Ù‚Ø±Ø§Ø± Ø¥Ø¯Ø§Ø±ÙŠ", "ØªØ¸Ù„Ù…", "Ø·Ø¹Ù†",
                "Ø¯ÙŠÙˆØ§Ù† Ø§Ù„Ù…Ø¸Ø§Ù„Ù…", "Ù…Ø­ÙƒÙ…Ø© Ø¥Ø¯Ø§Ø±ÙŠØ©", "Ù„Ø§Ø¦Ø­Ø©", "Ù†Ø¸Ø§Ù…",
                "Ù…ÙˆØ¸Ù Ø­ÙƒÙˆÙ…ÙŠ", "Ø®Ø¯Ù…Ø© Ù…Ø¯Ù†ÙŠØ©", "ØªØ±Ø®ÙŠØµ"
            ],
            LegalDomain.REAL_ESTATE: [
                "Ø¹Ù‚Ø§Ø±", "Ø£Ø±Ø¶", "ØµÙƒ", "Ù…Ù„ÙƒÙŠØ© Ø¹Ù‚Ø§Ø±ÙŠØ©", "Ø¨ÙŠØ¹ Ø¹Ù‚Ø§Ø±",
                "Ø¥ÙŠØ¬Ø§Ø± Ø¹Ù‚Ø§Ø±", "Ø±Ù‡Ù† Ø¹Ù‚Ø§Ø±ÙŠ", "ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø¹Ø¯Ù„", "Ø¥ÙØ±Ø§Øº",
                "ÙØ±Ø²", "ØªØ¬Ø²Ø¦Ø©", "ÙˆØ«ÙŠÙ‚Ø© Ø¹Ù‚Ø§Ø±ÙŠØ©", "Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¹ÙŠÙ†ÙŠ"
            ],
            LegalDomain.WAQF_INHERITANCE: [
                "Ù…ÙŠØ±Ø§Ø«", "ØªØ±ÙƒØ©", "ÙˆØ±Ø«Ø©", "ÙˆØ§Ø±Ø«", "ÙØ±ÙŠØ¶Ø©",
                "ÙˆÙ‚Ù", "ÙˆØµÙŠØ©", "Ø­ØµØ± Ø¥Ø±Ø«", "Ù‚Ø³Ù…Ø© ØªØ±ÙƒØ©",
                "Ù†ØµÙŠØ¨", "Ø¹ØµØ¨Ø©", "ÙØ±Ø¶", "ØªØ¹ØµÙŠØ¨"
            ],
            LegalDomain.EXECUTION: [
                "ØªÙ†ÙÙŠØ°", "Ø­ÙƒÙ…", "Ø³Ù†Ø¯ ØªÙ†ÙÙŠØ°ÙŠ", "Ù…Ø­ÙƒÙ…Ø© Ø§Ù„ØªÙ†ÙÙŠØ°",
                "Ø­Ø¬Ø²", "Ù…Ù†Ø¹ Ø³ÙØ±", "Ø¥ÙŠÙ‚Ø§Ù Ø®Ø¯Ù…Ø§Øª", "Ù…Ù‡Ù„Ø©",
                "Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª", "Ù…Ø±Ø§ÙØ¹Ø§Øª", "Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø±Ø§ÙØ¹Ø§Øª"
            ],
            LegalDomain.GENERAL_REGULATIONS: [
                "Ù†Ø¸Ø§Ù…", "Ù„Ø§Ø¦Ø­Ø©", "Ù‚Ø±Ø§Ø± ÙˆØ²Ø§Ø±ÙŠ", "Ù…Ø±Ø³ÙˆÙ… Ù…Ù„ÙƒÙŠ",
                "Ø£Ù…Ø± Ø³Ø§Ù…ÙŠ", "ØªØ¹Ù…ÙŠÙ…", "Ù‚ÙˆØ§Ø¹Ø¯", "Ø¶ÙˆØ§Ø¨Ø·"
            ]
        }
    
    def _initialize_patterns(self) -> Dict[LegalDomain, List[str]]:
        """Ø£Ù†Ù…Ø§Ø· regex Ù„ÙƒÙ„ Ù…Ø¬Ø§Ù„"""
        return {
            LegalDomain.PERSONAL_STATUS: [
                r"Ø­Ù‚\s*(Ø§Ù„Ø­Ø¶Ø§Ù†Ø©|Ø§Ù„Ø²ÙŠØ§Ø±Ø©|Ø§Ù„Ø±Ø¤ÙŠØ©)",
                r"(Ø£Ø¨|Ø£Ù…|ÙˆØ§Ù„Ø¯|ÙˆØ§Ù„Ø¯Ø©)\s*(Ø§Ù„Ù…Ø­Ø¶ÙˆÙ†|Ø§Ù„Ø·ÙÙ„)",
                r"Ù†ÙÙ‚Ø©\s*(Ø§Ù„Ø£ÙˆÙ„Ø§Ø¯|Ø§Ù„Ø²ÙˆØ¬Ø©|Ø§Ù„Ù…Ø·Ù„Ù‚Ø©)"
            ],
            LegalDomain.CRIMINAL: [
                r"(Ø§ØªÙ‡Ø§Ù…|ØªÙ‡Ù…Ø©)\s*Ø¨",
                r"Ø¹Ù‚ÙˆØ¨Ø©\s*(Ø§Ù„Ø³Ø¬Ù†|Ø§Ù„Ø­Ø¨Ø³|Ø§Ù„ØºØ±Ø§Ù…Ø©)",
                r"Ø¬Ø±ÙŠÙ…Ø©\s*(Ø§Ù„Ù‚ØªÙ„|Ø§Ù„Ø³Ø±Ù‚Ø©|Ø§Ù„Ø§Ø­ØªÙŠØ§Ù„)"
            ],
            LegalDomain.CIVIL: [
                r"Ø¹Ù‚Ø¯\s*(Ø¨ÙŠØ¹|Ø¥ÙŠØ¬Ø§Ø±|Ø´Ø±Ø§Ø¡)",
                r"(ÙØ³Ø®|Ø¥Ù„ØºØ§Ø¡)\s*Ø§Ù„Ø¹Ù‚Ø¯",
                r"ØªØ¹ÙˆÙŠØ¶\s*Ø¹Ù†\s*(Ø§Ù„Ø¶Ø±Ø±|Ø§Ù„Ø®Ø³Ø§Ø±Ø©)"
            ],
            LegalDomain.LABOR: [
                r"Ø¹Ù‚Ø¯\s*Ø¹Ù…Ù„",
                r"(ÙØµÙ„|Ø¥Ù†Ù‡Ø§Ø¡\s*Ø®Ø¯Ù…Ø§Øª)\s*(ØªØ¹Ø³ÙÙŠ)?",
                r"Ù…ÙƒØ§ÙØ£Ø©\s*Ù†Ù‡Ø§ÙŠØ©\s*Ø§Ù„Ø®Ø¯Ù…Ø©"
            ]
        }
    
    def _initialize_weights(self) -> Dict[str, float]:
        """Ø£ÙˆØ²Ø§Ù† Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„ØªØ­Ø¯ÙŠØ¯"""
        return {
            "keyword_match": 0.4,
            "pattern_match": 0.3,
            "synonym_match": 0.2,
            "context_match": 0.1
        }
    
    def detect(self, question: str) -> List[DomainScore]:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¬Ø§Ù„ Ù…Ø¹ Ø§Ù„Ø¯Ø±Ø¬Ø§Øª"""
        scores = []
        
        for domain, keywords in self.domain_keywords.items():
            if domain == LegalDomain.UNKNOWN:
                continue
            
            score = 0.0
            matched = []
            
            # Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª
            keyword_matches = self.fuzzy_matcher.find_matches(question, keywords, domain)
            if keyword_matches:
                exact_matches = [m for m in keyword_matches if m.is_exact]
                fuzzy_matches = [m for m in keyword_matches if not m.is_exact]
                
                score += len(exact_matches) * self.domain_weights["keyword_match"]
                score += len(fuzzy_matches) * self.domain_weights["synonym_match"]
                matched.extend([m.keyword for m in keyword_matches])
            
            # Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø£Ù†Ù…Ø§Ø·
            if domain in self.domain_patterns:
                for pattern in self.domain_patterns[domain]:
                    if re.search(pattern, question):
                        score += self.domain_weights["pattern_match"]
                        matched.append(f"pattern:{pattern[:20]}")
            
            if score > 0:
                confidence = min(1.0, score / 3)  # normalize
                scores.append(DomainScore(
                    domain=domain,
                    score=score,
                    matched_keywords=matched,
                    confidence=confidence,
                    reasoning=f"ØªØ·Ø§Ø¨Ù‚ {len(matched)} ÙƒÙ„Ù…Ø©/Ù†Ù…Ø·"
                ))
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ù†ØªÙŠØ¬Ø©
        scores.sort(key=lambda x: x.score, reverse=True)
        return scores
    
    def get_primary_domain(self, question: str) -> Tuple[LegalDomain, float]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¬Ø§Ù„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ"""
        scores = self.detect(question)
        if scores:
            return scores[0].domain, scores[0].confidence
        return LegalDomain.UNKNOWN, 0.0


# ============================================================
# COMPLEXITY ANALYZER - Ù…Ø­Ù„Ù„ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
# ============================================================

class ComplexityAnalyzer:
    """Ù…Ø­Ù„Ù„ ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
    
    def __init__(self):
        self.indicators = self._initialize_indicators()
        self.thresholds = {
            "simple": 3,
            "moderate": 7,
            "complex": 12,
            "expert": float('inf')
        }
    
    def _initialize_indicators(self) -> Dict[str, Dict[str, Any]]:
        """Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ØªØ¹Ù‚ÙŠØ¯"""
        return {
            "conditionals": {
                "patterns": ["Ø¥Ø°Ø§", "ÙÙŠ Ø­Ø§Ù„Ø©", "Ø¨Ø´Ø±Ø·", "Ø¹Ù†Ø¯", "Ù„Ùˆ", "Ù…ØªÙ‰"],
                "weight": 2.0
            },
            "exceptions": {
                "patterns": ["Ø¥Ù„Ø§", "Ù…Ø§Ø¹Ø¯Ø§", "Ø¨Ø§Ø³ØªØ«Ù†Ø§Ø¡", "Ù„ÙƒÙ†", "ØºÙŠØ± Ø£Ù†"],
                "weight": 2.5
            },
            "multiple_aspects": {
                "patterns": ["Ùˆ", "Ø£ÙŠØ¶Ø§Ù‹", "Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ©", "ÙƒÙ…Ø§", "ÙØ¶Ù„Ø§Ù‹"],
                "weight": 1.0
            },
            "comparison": {
                "patterns": ["Ø§Ù„ÙØ±Ù‚", "Ù‚Ø§Ø±Ù†", "Ø£ÙŠÙ‡Ù…Ø§", "Ù…Ù‚Ø§Ø±Ù†Ø©"],
                "weight": 2.0
            },
            "explanation": {
                "patterns": ["ÙƒÙŠÙ", "Ù„Ù…Ø§Ø°Ø§", "ÙˆØ¶Ø­", "Ø§Ø´Ø±Ø­", "ÙØ³Ù‘Ø±"],
                "weight": 1.5
            },
            "legal_terms": {
                "patterns": ["Ø­Ø³Ø¨ Ø§Ù„Ù†Ø¸Ø§Ù…", "ÙˆÙÙ‚Ø§Ù‹ Ù„Ù€", "Ø¨Ù…ÙˆØ¬Ø¨", "Ø·Ø¨Ù‚Ø§Ù‹"],
                "weight": 1.5
            },
            "multi_party": {
                "patterns": ["ÙˆØ§Ù„Ù…Ø¯Ø¹Ù‰ Ø¹Ù„ÙŠÙ‡", "Ø§Ù„Ø·Ø±ÙÙŠÙ†", "ÙƒÙ„Ø§", "Ø¬Ù…ÙŠØ¹"],
                "weight": 1.5
            },
            "temporal": {
                "patterns": ["Ù‚Ø¨Ù„", "Ø¨Ø¹Ø¯", "Ø®Ù„Ø§Ù„", "Ù…Ù†Ø°", "Ø­ØªÙ‰"],
                "weight": 1.0
            }
        }
    
    def analyze(self, question: str) -> Tuple[QuestionComplexity, Dict[str, float]]:
        """ØªØ­Ù„ÙŠÙ„ ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ø³Ø¤Ø§Ù„"""
        scores = {}
        total_score = 0.0
        
        for indicator_name, config in self.indicators.items():
            patterns = config["patterns"]
            weight = config["weight"]
            
            count = sum(1 for p in patterns if p in question)
            score = count * weight
            
            scores[indicator_name] = score
            total_score += score
        
        # Ø·ÙˆÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„
        word_count = len(question.split())
        length_score = word_count / 10  # 10 ÙƒÙ„Ù…Ø§Øª = 1 Ù†Ù‚Ø·Ø©
        scores["length"] = length_score
        total_score += length_score
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³ØªÙˆÙ‰
        if total_score < self.thresholds["simple"]:
            complexity = QuestionComplexity.SIMPLE
        elif total_score < self.thresholds["moderate"]:
            complexity = QuestionComplexity.MODERATE
        elif total_score < self.thresholds["complex"]:
            complexity = QuestionComplexity.COMPLEX
        else:
            complexity = QuestionComplexity.EXPERT
        
        return complexity, scores


# ============================================================
# INTENT DETECTOR - ÙƒØ§Ø´Ù Ø§Ù„Ù†ÙŠØ©
# ============================================================

class IntentDetector:
    """ÙƒØ§Ø´Ù Ù†ÙŠØ© Ø§Ù„Ø³Ø¤Ø§Ù„"""
    
    def __init__(self):
        self.intents = {
            "definition": {
                "patterns": ["Ù…Ø§ Ù‡Ùˆ", "Ù…Ø§ Ù‡ÙŠ", "Ù…Ø§ Ø§Ù„Ù…Ù‚ØµÙˆØ¯", "Ø¹Ø±Ù‘Ù", "ØªØ¹Ø±ÙŠÙ"],
                "description": "Ø·Ù„Ø¨ ØªØ¹Ø±ÙŠÙ"
            },
            "procedure": {
                "patterns": ["ÙƒÙŠÙ", "Ù…Ø§ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª", "Ù…Ø§ Ø§Ù„Ø®Ø·ÙˆØ§Øª", "Ø·Ø±ÙŠÙ‚Ø©"],
                "description": "Ø·Ù„Ø¨ Ø¥Ø¬Ø±Ø§Ø¡"
            },
            "eligibility": {
                "patterns": ["Ù‡Ù„ ÙŠØ­Ù‚", "Ù‡Ù„ ÙŠØ¬ÙˆØ²", "Ù‡Ù„ Ø£Ø³ØªØ·ÙŠØ¹", "Ù‡Ù„ ÙŠÙ…ÙƒÙ†"],
                "description": "Ø§Ø³ØªÙØ³Ø§Ø± Ø¹Ù† Ø§Ù„Ø£Ù‡Ù„ÙŠØ©"
            },
            "comparison": {
                "patterns": ["Ù…Ø§ Ø§Ù„ÙØ±Ù‚", "Ù‚Ø§Ø±Ù†", "Ø£ÙŠÙ‡Ù…Ø§ Ø£ÙØ¶Ù„"],
                "description": "Ø·Ù„Ø¨ Ù…Ù‚Ø§Ø±Ù†Ø©"
            },
            "consequence": {
                "patterns": ["Ù…Ø§Ø°Ø§ ÙŠØ­Ø¯Ø«", "Ù…Ø§ Ø§Ù„Ù†ØªÙŠØ¬Ø©", "Ù…Ø§ Ø§Ù„Ø¹Ù‚ÙˆØ¨Ø©"],
                "description": "Ø§Ø³ØªÙØ³Ø§Ø± Ø¹Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬"
            },
            "deadline": {
                "patterns": ["Ù…ØªÙ‰", "Ù…Ø§ Ø§Ù„Ù…Ø¯Ø©", "ÙƒÙ… ÙŠÙˆÙ…", "Ù…Ø§ Ø§Ù„Ù…ÙˆØ¹Ø¯"],
                "description": "Ø§Ø³ØªÙØ³Ø§Ø± Ø¹Ù† Ù…ÙˆØ§Ø¹ÙŠØ¯"
            },
            "advice": {
                "patterns": ["Ù…Ø§Ø°Ø§ Ø£ÙØ¹Ù„", "Ø£Ù†ØµØ­Ù†ÙŠ", "Ù…Ø§ Ø±Ø£ÙŠÙƒ"],
                "description": "Ø·Ù„Ø¨ Ù†ØµÙŠØ­Ø©"
            }
        }
    
    def detect(self, question: str) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†ÙŠØ©"""
        for intent_name, config in self.intents.items():
            if any(p in question for p in config["patterns"]):
                return intent_name
        return "general"


# ============================================================
# ENTITY EXTRACTOR - Ù…Ø³ØªØ®Ø±Ø¬ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª
# ============================================================

class EntityExtractor:
    """Ù…Ø³ØªØ®Ø±Ø¬ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©"""
    
    def __init__(self):
        self.entity_patterns = {
            "Ù…Ø¨Ø§Ù„Øº": r'(\d+(?:,\d{3})*(?:\.\d+)?)\s*(Ø±ÙŠØ§Ù„|Ø¯ÙˆÙ„Ø§Ø±|Ø¬Ù†ÙŠÙ‡)',
            "Ù…Ø¯Ø¯": r'(\d+)\s*(ÙŠÙˆÙ…|Ø´Ù‡Ø±|Ø³Ù†Ø©|Ø£Ø³Ø¨ÙˆØ¹)',
            "Ù†Ø³Ø¨": r'(\d+(?:\.\d+)?)\s*[%Ùª]',
            "Ø£Ø¹Ù…Ø§Ø±": r'(\d+)\s*(?:Ø³Ù†Ø©|Ø¹Ø§Ù…)(?:\s*(?:Ù‡Ø¬Ø±ÙŠ|Ù…ÙŠÙ„Ø§Ø¯ÙŠ))?',
            "ØªÙˆØ§Ø±ÙŠØ®": r'\d{1,2}[/-]\d{1,2}[/-]\d{2,4}',
            "Ù…ÙˆØ§Ø¯_Ù†Ø¸Ø§Ù…ÙŠØ©": r'(?:Ø§Ù„Ù…Ø§Ø¯Ø©|Ù…Ø§Ø¯Ø©)\s*(?:Ø±Ù‚Ù…\s*)?(\d+)',
        }
    
    def extract(self, text: str) -> Dict[str, List[str]]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª"""
        entities = {}
        
        for entity_type, pattern in self.entity_patterns.items():
            matches = re.findall(pattern, text)
            if matches:
                if isinstance(matches[0], tuple):
                    entities[entity_type] = [" ".join(m) for m in matches]
                else:
                    entities[entity_type] = matches
        
        return entities


# ============================================================
# FEW-SHOT MANAGER - Ù…Ø¯ÙŠØ± Ø§Ù„Ø£Ù…Ø«Ù„Ø©
# ============================================================

class FewShotManager:
    """Ù…Ø¯ÙŠØ± Ø£Ù…Ø«Ù„Ø© Ø§Ù„ØªØ¹Ù„Ù…"""
    
    def __init__(self):
        self.examples: Dict[LegalDomain, List[FewShotExample]] = defaultdict(list)
        self._initialize_examples()
    
    def _initialize_examples(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ©"""
        # Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ø£Ø­ÙˆØ§Ù„ Ø§Ù„Ø´Ø®ØµÙŠØ©
        self.examples[LegalDomain.PERSONAL_STATUS] = [
            FewShotExample(
                question="Ù…ØªÙ‰ ØªØ³Ù‚Ø· Ø­Ø¶Ø§Ù†Ø© Ø§Ù„Ø£Ù…ØŸ",
                answer="""ØªØ³Ù‚Ø· Ø­Ø¶Ø§Ù†Ø© Ø§Ù„Ø£Ù… ÙÙŠ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©:
1. Ø²ÙˆØ§Ø¬Ù‡Ø§ Ù…Ù† Ø£Ø¬Ù†Ø¨ÙŠ Ø¹Ù† Ø§Ù„Ù…Ø­Ø¶ÙˆÙ†
2. Ø¥Ù‡Ù…Ø§Ù„Ù‡Ø§ Ù„Ù„Ù…Ø­Ø¶ÙˆÙ† Ø¨Ù…Ø§ ÙŠØ¶Ø±Ù‡
3. Ø¥ØµØ§Ø¨ØªÙ‡Ø§ Ø¨Ù…Ø±Ø¶ Ù…Ø¹Ø¯Ù Ø®Ø·ÙŠØ±
4. Ø³ÙˆØ¡ Ø®Ù„Ù‚Ù‡Ø§ Ø§Ù„Ø¸Ø§Ù‡Ø±
5. Ø¹Ø¯Ù… Ø£Ù…Ø§Ù†ØªÙ‡Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­Ø¶ÙˆÙ†

**Ø§Ù„Ù…ØµØ¯Ø±:** Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø­ÙˆØ§Ù„ Ø§Ù„Ø´Ø®ØµÙŠØ©ØŒ Ø§Ù„Ù…Ø§Ø¯Ø© 127""",
                domain=LegalDomain.PERSONAL_STATUS,
                quality_score=0.95,
                source="expert_validated"
            ),
            FewShotExample(
                question="ÙƒÙ… Ù…Ø¯Ø© Ø­Ø¶Ø§Ù†Ø© Ø§Ù„Ø£Ù… Ù„Ù„Ø·ÙÙ„ Ø§Ù„Ø°ÙƒØ±ØŸ",
                answer="""Ø­Ø³Ø¨ Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø­ÙˆØ§Ù„ Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ:
- Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø°ÙƒØ±: ØªØ³ØªÙ…Ø± Ø§Ù„Ø­Ø¶Ø§Ù†Ø© Ø­ØªÙ‰ ÙŠØ¨Ù„Øº **Ø³Ø¨Ø¹ Ø³Ù†ÙˆØ§Øª**
- Ø¨Ø¹Ø¯Ù‡Ø§ ÙŠÙØ®ÙŠÙÙ‘Ø± Ø¨ÙŠÙ† Ø£Ø¨ÙˆÙŠÙ‡
- ÙŠÙ…ÙƒÙ† Ù„Ù„Ù…Ø­ÙƒÙ…Ø© ØªÙ…Ø¯ÙŠØ¯Ù‡Ø§ Ù„Ù…ØµÙ„Ø­Ø© Ø§Ù„Ù…Ø­Ø¶ÙˆÙ†

**Ø§Ù„Ù…ØµØ¯Ø±:** Ø§Ù„Ù…Ø§Ø¯Ø© 126 Ù…Ù† Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø­ÙˆØ§Ù„ Ø§Ù„Ø´Ø®ØµÙŠØ©""",
                domain=LegalDomain.PERSONAL_STATUS,
                quality_score=0.9,
                source="expert_validated"
            )
        ]
        
        # Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ø¹Ù…Ù„
        self.examples[LegalDomain.LABOR] = [
            FewShotExample(
                question="ÙƒÙ… Ù…ÙƒØ§ÙØ£Ø© Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø®Ø¯Ù…Ø© Ù„Ù„Ø¹Ø§Ù…Ù„ØŸ",
                answer="""ØªÙØ­Ø³Ø¨ Ù…ÙƒØ§ÙØ£Ø© Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø®Ø¯Ù…Ø© ÙƒØ§Ù„ØªØ§Ù„ÙŠ:

**Ù„Ù„Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ù…Ø³ Ø§Ù„Ø£ÙˆÙ„Ù‰:**
- Ù†ØµÙ Ø´Ù‡Ø± Ø¹Ù† ÙƒÙ„ Ø³Ù†Ø©

**Ù„Ù„Ø³Ù†ÙˆØ§Øª Ø§Ù„ØªÙŠ ØªØ²ÙŠØ¯ Ø¹Ù† Ø®Ù…Ø³:**
- Ø´Ù‡Ø± ÙƒØ§Ù…Ù„ Ø¹Ù† ÙƒÙ„ Ø³Ù†Ø©

**Ù…Ù„Ø§Ø­Ø¸Ø©:** ÙŠØ³ØªØ­Ù‚ Ø§Ù„Ø¹Ø§Ù…Ù„ Ø§Ù„Ù…Ø³ØªÙ‚ÙŠÙ„:
- Ø«Ù„Ø« Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©: 2-5 Ø³Ù†ÙˆØ§Øª
- Ø«Ù„Ø«ÙŠ Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©: 5-10 Ø³Ù†ÙˆØ§Øª
- Ø§Ù„Ù…ÙƒØ§ÙØ£Ø© ÙƒØ§Ù…Ù„Ø©: Ø£ÙƒØ«Ø± Ù…Ù† 10 Ø³Ù†ÙˆØ§Øª

**Ø§Ù„Ù…ØµØ¯Ø±:** Ø§Ù„Ù…Ø§Ø¯Ø© 84 Ùˆ 85 Ù…Ù† Ù†Ø¸Ø§Ù… Ø§Ù„Ø¹Ù…Ù„""",
                domain=LegalDomain.LABOR,
                quality_score=0.95,
                source="expert_validated"
            )
        ]
    
    def get_examples(
        self,
        domain: LegalDomain,
        max_examples: int = 2,
        min_quality: float = 0.8
    ) -> List[FewShotExample]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ù…Ø«Ù„Ø©"""
        examples = self.examples.get(domain, [])
        
        # ØªØµÙÙŠØ© Ø­Ø³Ø¨ Ø§Ù„Ø¬ÙˆØ¯Ø©
        filtered = [e for e in examples if e.quality_score >= min_quality]
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø¬ÙˆØ¯Ø©
        filtered.sort(key=lambda x: x.quality_score, reverse=True)
        
        return filtered[:max_examples]
    
    def format_examples(self, examples: List[FewShotExample]) -> str:
        """ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ù„Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª"""
        if not examples:
            return ""
        
        formatted = "\n**Ø£Ù…Ø«Ù„Ø© ØªÙˆØ¶ÙŠØ­ÙŠØ©:**\n"
        
        for i, ex in enumerate(examples, 1):
            formatted += f"\n--- Ù…Ø«Ø§Ù„ {i} ---\n"
            formatted += f"**Ø§Ù„Ø³Ø¤Ø§Ù„:** {ex.question}\n"
            formatted += f"**Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:**\n{ex.answer}\n"
        
        formatted += "\n--- Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø£Ù…Ø«Ù„Ø© ---\n\n"
        return formatted
    
    def add_example(self, example: FewShotExample):
        """Ø¥Ø¶Ø§ÙØ© Ù…Ø«Ø§Ù„ Ø¬Ø¯ÙŠØ¯"""
        self.examples[example.domain].append(example)


# ============================================================
# PROMPT TEMPLATE MANAGER - Ù…Ø¯ÙŠØ± Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨
# ============================================================

class PromptTemplateManager:
    """Ù…Ø¯ÙŠØ± Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª Ù…Ø¹ A/B Testing"""
    
    def __init__(self):
        self.templates: Dict[str, List[PromptTemplate]] = defaultdict(list)
        self.performance: Dict[str, PromptPerformance] = {}
        self._initialize_templates()
    
    def _initialize_templates(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨"""
        
        # Ù‚ÙˆØ§Ù„Ø¨ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„
        self.templates["question_analyzer"] = [
            PromptTemplate(
                template_id="qa_v1",
                name="Ù…Ø­Ù„Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ",
                version="1.0",
                template="""Ø£Ù†Øª Ù…Ø­Ù„Ù„ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØªØ®ØµØµ. Ø­Ù„Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ:

**Ø§Ù„Ø³Ø¤Ø§Ù„:** {question}

**Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:**
1. Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© (5-10 ÙƒÙ„Ù…Ø§Øª)
2. Ø­Ø¯Ø¯ Ø§Ù„Ù…Ø¬Ø§Ù„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ
3. Ù‚ÙŠÙ‘Ù… Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
4. Ø§Ù‚ØªØ±Ø­ Ø£Ø³Ø¦Ù„Ø© ÙØ±Ø¹ÙŠØ© Ù„Ù„Ø¨Ø­Ø«

**Ø£Ø¬Ø¨ Ø¨Ù€ JSON:**
{{
    "keywords": ["ÙƒÙ„Ù…Ø©1", "ÙƒÙ„Ù…Ø©2"],
    "domain": "Ø§Ù„Ù…Ø¬Ø§Ù„",
    "complexity": "simple/moderate/complex",
    "sub_questions": ["Ø³Ø¤Ø§Ù„ ÙØ±Ø¹ÙŠ"]
}}""",
                variables=["question"],
                category="analysis",
                style=PromptStyle.FORMAL
            ),
            PromptTemplate(
                template_id="qa_v2",
                name="Ù…Ø­Ù„Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø­Ø³Ù‘Ù†",
                version="2.0",
                template="""Ø¨ØµÙØªÙƒ Ø®Ø¨ÙŠØ±Ø§Ù‹ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ´Ø§Ø±Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©:

{magic_words}

**Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø·Ø±ÙˆØ­:**
{question}

**Ø­Ù„Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„ ÙˆØ§Ø³ØªØ®Ø±Ø¬:**
1. Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©
2. Ø§Ù„Ù…Ø¬Ø§Ù„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ÙˆØ§Ù„ÙØ±Ø¹ÙŠ
3. Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ ÙˆØ§Ù„Ø³Ø¨Ø¨
4. Ø§Ù„Ù†ÙŠØ© Ù…Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ (Ø§Ø³ØªÙØ³Ø§Ø±/Ø·Ù„Ø¨ Ø¥Ø¬Ø±Ø§Ø¡/Ù†ØµÙŠØ­Ø©)
5. Ø£Ø³Ø¦Ù„Ø© ÙØ±Ø¹ÙŠØ© ØªØ³Ø§Ø¹Ø¯ ÙÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø©

**Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ù€ JSON:**
{{
    "keywords": [],
    "primary_domain": "",
    "secondary_domain": "",
    "complexity": "",
    "complexity_reason": "",
    "intent": "",
    "sub_questions": []
}}""",
                variables=["question", "magic_words"],
                category="analysis",
                style=PromptStyle.FORMAL
            )
        ]
        
        # Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ
        self.templates["legal_reasoning"] = [
            PromptTemplate(
                template_id="lr_v1",
                name="Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ",
                version="1.0",
                template="""Ø£Ù†Øª Ù…Ø­Ø§Ù…Ù Ø³Ø¹ÙˆØ¯ÙŠ Ø®Ø¨ÙŠØ± ÙÙŠ {domain}.

{magic_words}

**Ø§Ù„Ø³Ø¤Ø§Ù„:** {question}

**Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©:**
{context}

**Ù‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø© Ø´Ø§Ù…Ù„Ø© ØªØªØ¶Ù…Ù†:**
1. Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ù†Ø·Ø¨Ù‚Ø©
2. ØªØ·Ø¨ÙŠÙ‚Ù‡Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø§Ù„Ø©
3. Ø§Ù„Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª Ø¥Ù† ÙˆØ¬Ø¯Øª
4. Ø§Ù„ØªÙˆØµÙŠØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©

**Ù…Ù„Ø§Ø­Ø¸Ø©:** Ø§Ø³ØªØ´Ù‡Ø¯ Ø¨Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù†Ø¸Ø§Ù…ÙŠØ© Ø­ÙŠØ« Ø£Ù…ÙƒÙ†.""",
                variables=["domain", "magic_words", "question", "context"],
                category="reasoning",
                style=PromptStyle.FORMAL
            ),
            PromptTemplate(
                template_id="lr_v2",
                name="Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…",
                version="2.0",
                template="""{expert_role}

{magic_words}

**Ø§Ù„Ù‚Ø¶ÙŠØ© Ø§Ù„Ù…Ø·Ø±ÙˆØ­Ø©:**
{question}

**Ø§Ù„Ø³ÙŠØ§Ù‚ ÙˆØ§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª:**
{context}

{few_shot_examples}

**Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª:**
1. Ø­Ø¯Ø¯ Ø§Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ù…Ù†Ø·Ø¨Ù‚ Ø¨Ø¯Ù‚Ø©
2. Ø­Ù„Ù„ Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹ ÙÙŠ Ø¶ÙˆØ¡ Ø§Ù„Ù†Ø¸Ø§Ù…
3. Ù†Ø§Ù‚Ø´ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
4. Ù‚Ø¯Ù… Ø§Ù„Ø±Ø£ÙŠ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…Ø¹ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø©
5. Ø§Ø°ÙƒØ± Ø§Ù„Ù…Ø­Ø§Ø°ÙŠØ± ÙˆØ§Ù„Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª

**Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©:**
- **Ø§Ù„Ø®Ù„Ø§ØµØ©:** (Ø¬Ù…Ù„Ø© ÙˆØ§Ø­Ø¯Ø©)
- **Ø§Ù„ØªØ­Ù„ÙŠÙ„:** (Ù…ÙØµÙ„)
- **Ø§Ù„Ø£Ø³Ø§Ø³ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ:** (Ø§Ù„Ù…ÙˆØ§Ø¯ ÙˆØ§Ù„Ø£Ù†Ø¸Ù…Ø©)
- **Ø§Ù„ØªÙˆØµÙŠØ§Øª:** (Ø¹Ù…Ù„ÙŠØ©)
- **Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª:** (Ø¥Ù† ÙˆØ¬Ø¯Øª)
- **Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø©:** (Ø¹Ø§Ù„ÙŠØ©/Ù…ØªÙˆØ³Ø·Ø©/Ù…Ù†Ø®ÙØ¶Ø©)""",
                variables=["expert_role", "magic_words", "question", "context", "few_shot_examples"],
                category="reasoning",
                style=PromptStyle.FORMAL
            )
        ]
        
        # Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„ØªØ­Ù‚Ù‚
        self.templates["verification"] = [
            PromptTemplate(
                template_id="ver_v1",
                name="Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ",
                version="1.0",
                template="""Ø£Ù†Øª Ù…Ø¯Ù‚Ù‚ Ù‚Ø§Ù†ÙˆÙ†ÙŠ ØµØ§Ø±Ù…. Ø±Ø§Ø¬Ø¹ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:

**Ø§Ù„Ø³Ø¤Ø§Ù„:** {question}
**Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:** {answer}
**Ø§Ù„Ù…ØµØ§Ø¯Ø±:** {sources_count}

**ØªØ­Ù‚Ù‚ Ù…Ù†:**
1. Ø¯Ù‚Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
2. ØµØ­Ø© Ø§Ù„Ù…Ù†Ø·Ù‚
3. Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
4. ÙˆØ¬ÙˆØ¯ ØªÙ†Ø§Ù‚Ø¶Ø§Øª

**Ø£Ø¬Ø¨ Ø¨Ù€ JSON:**
{{
    "is_valid": true/false,
    "confidence": 0.0-1.0,
    "issues": [],
    "suggestions": [],
    "verdict": "Ù…Ù‚Ø¨ÙˆÙ„/ÙŠØ­ØªØ§Ø¬ ØªØ¹Ø¯ÙŠÙ„/Ù…Ø±ÙÙˆØ¶"
}}""",
                variables=["question", "answer", "sources_count"],
                category="verification",
                style=PromptStyle.FORMAL
            )
        ]
    
    def get_template(
        self,
        category: str,
        version: Optional[str] = None,
        use_ab_testing: bool = True
    ) -> PromptTemplate:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚Ø§Ù„Ø¨"""
        templates = self.templates.get(category, [])
        
        if not templates:
            raise ValueError(f"No templates for category: {category}")
        
        if version:
            for t in templates:
                if t.version == version:
                    return t
        
        if use_ab_testing and len(templates) > 1:
            # Ø§Ø®ØªÙŠØ§Ø± Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ù…ÙˆØ²ÙˆÙ† Ø­Ø³Ø¨ Ø§Ù„Ø£Ø¯Ø§Ø¡
            weights = [max(0.1, t.performance_score) for t in templates]
            total = sum(weights)
            probs = [w/total for w in weights]
            return np.random.choice(templates, p=probs)
        
        return templates[0]
    
    def record_performance(
        self,
        template_id: str,
        success: bool,
        quality: float,
        response_time: float
    ):
        """ØªØ³Ø¬ÙŠÙ„ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù‚Ø§Ù„Ø¨"""
        if template_id not in self.performance:
            self.performance[template_id] = PromptPerformance(
                template_id=template_id,
                success_rate=0.0,
                avg_response_quality=0.0,
                avg_response_time=0.0,
                usage_count=0,
                last_used=datetime.now()
            )
        
        perf = self.performance[template_id]
        perf.usage_count += 1
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…ØªÙˆØ³Ø·Ø§Øª
        n = perf.usage_count
        perf.success_rate = ((n-1) * perf.success_rate + (1 if success else 0)) / n
        perf.avg_response_quality = ((n-1) * perf.avg_response_quality + quality) / n
        perf.avg_response_time = ((n-1) * perf.avg_response_time + response_time) / n
        perf.last_used = datetime.now()
        
        # ØªØ­Ø¯ÙŠØ« Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙÙŠ Ø§Ù„Ù‚Ø§Ù„Ø¨
        for category_templates in self.templates.values():
            for template in category_templates:
                if template.template_id == template_id:
                    template.performance_score = perf.success_rate * 0.5 + perf.avg_response_quality * 0.5
                    template.usage_count = perf.usage_count


# ============================================================
# ADVANCED QUERY GENERATOR - Ù…ÙˆÙ„Ø¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
# ============================================================

class AdvancedQueryGenerator:
    """Ù…ÙˆÙ„Ø¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
    
    def __init__(self):
        self.domain_detector = SemanticDomainDetector()
        self.complexity_analyzer = ComplexityAnalyzer()
        self.intent_detector = IntentDetector()
        self.entity_extractor = EntityExtractor()
        self.fuzzy_matcher = FuzzyKeywordMatcher()
        
        logger.info("ğŸ§  AdvancedQueryGenerator initialized")
    
    def analyze(self, question: str) -> AnalyzedQuestion:
        """ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„Ø³Ø¤Ø§Ù„"""
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ
        normalized = self._normalize_text(question)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
        keywords = self._extract_keywords(normalized)
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¬Ø§Ù„
        domain_scores = self.domain_detector.detect(normalized)
        primary_domain = domain_scores[0].domain if domain_scores else LegalDomain.UNKNOWN
        secondary_domains = [ds.domain for ds in domain_scores[1:3] if ds.score > 0.5]
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
        legal_keywords = self._extract_legal_keywords(normalized, primary_domain)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
        complexity, complexity_factors = self.complexity_analyzer.analyze(normalized)
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†ÙŠØ©
        intent = self.intent_detector.detect(normalized)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª
        entities = self.entity_extractor.extract(normalized)
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ©
        sub_questions = self._generate_sub_questions(question, primary_domain, intent)
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø¨Ø­Ø«
        search_queries = self._generate_search_queries(
            legal_keywords, primary_domain, intent
        )
        
        logger.info(f"ğŸ“Š Analyzed: domain={primary_domain.arabic_name}, "
                   f"complexity={complexity.key}, intent={intent}")
        
        return AnalyzedQuestion(
            original=question,
            normalized=normalized,
            keywords=keywords,
            legal_keywords=legal_keywords,
            domain=primary_domain,
            domain_scores=domain_scores,
            secondary_domains=secondary_domains,
            complexity=complexity,
            complexity_factors=complexity_factors,
            sub_questions=sub_questions,
            search_queries=search_queries,
            intent=intent,
            entities=entities
        )
    
    def _normalize_text(self, text: str) -> str:
        """ØªÙ†Ø¸ÙŠÙ ÙˆØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù†Øµ"""
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„
        text = re.sub(r'[\u064B-\u065F]', '', text)
        
        # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù‡Ù…Ø²Ø§Øª
        text = re.sub(r'[Ø¥Ø£Ø¢Ø§]', 'Ø§', text)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
        text = re.sub(r'[ØŸ?!]+', 'ØŸ', text)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def _extract_keywords(self, text: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©"""
        words = re.findall(r'[\u0600-\u06FF]+', text)
        
        stop_words = {
            "ÙÙŠ", "Ù…Ù†", "Ø¥Ù„Ù‰", "Ø¹Ù„Ù‰", "Ø¹Ù†", "Ù‡Ù„", "Ù…Ø§", "Ù‡ÙŠ", "Ù‡Ùˆ",
            "Ø£Ù†", "ÙƒØ§Ù†", "Ø§Ù„ØªÙŠ", "Ø§Ù„Ø°ÙŠ", "Ù‡Ø°Ø§", "Ù‡Ø°Ù‡", "Ù…Ø¹", "Ø£Ùˆ",
            "ÙƒÙ„", "Ø¨Ø¹Ø¯", "Ù‚Ø¨Ù„", "Ø£ÙŠ", "Ù„Ù…", "Ù„Ø§", "Ù‚Ø¯", "ÙƒÙŠÙ"
        }
        
        keywords = [w for w in words if len(w) > 2 and w not in stop_words]
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ØªØ±ØªÙŠØ¨
        seen = set()
        unique = []
        for kw in keywords:
            if kw not in seen:
                seen.add(kw)
                unique.append(kw)
        
        return unique[:15]
    
    def _extract_legal_keywords(
        self,
        text: str,
        domain: LegalDomain
    ) -> List[KeywordMatch]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©"""
        keywords = self.domain_detector.domain_keywords.get(domain, [])
        
        # ØªÙˆØ³ÙŠØ¹ Ø¨Ø§Ù„Ù…ØªØ±Ø§Ø¯ÙØ§Øª
        expanded = self.fuzzy_matcher.expand_keywords(keywords)
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø§Øª
        matches = self.fuzzy_matcher.find_matches(text, expanded, domain)
        
        return matches
    
    def _generate_sub_questions(
        self,
        question: str,
        domain: LegalDomain,
        intent: str
    ) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ Ø£Ø³Ø¦Ù„Ø© ÙØ±Ø¹ÙŠØ© Ø°ÙƒÙŠØ©"""
        sub_questions = []
        
        # Ø£Ø³Ø¦Ù„Ø© Ø­Ø³Ø¨ Ø§Ù„Ù…Ø¬Ø§Ù„
        domain_questions = {
            LegalDomain.PERSONAL_STATUS: {
                "Ø­Ø¶Ø§Ù†Ø©": [
                    "Ù…Ø§ Ø´Ø±ÙˆØ· Ø§Ù„Ø­Ø¶Ø§Ù†Ø© ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØŸ",
                    "Ù…Ø§ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„ØªÙŠ ØªØ³Ù‚Ø· ÙÙŠÙ‡Ø§ Ø§Ù„Ø­Ø¶Ø§Ù†Ø©ØŸ",
                    "ÙƒÙŠÙ ÙŠØªÙ… ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ø²ÙŠØ§Ø±Ø©ØŸ"
                ],
                "Ø·Ù„Ø§Ù‚": [
                    "Ù…Ø§ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø·Ù„Ø§Ù‚ ÙˆØ´Ø±ÙˆØ· ÙƒÙ„ Ù…Ù†Ù‡Ø§ØŸ",
                    "Ù…Ø§ Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ù…Ø·Ù„Ù‚Ø©ØŸ",
                    "Ù…Ø§ Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø·Ù„Ø§Ù‚ Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠØŸ"
                ],
                "Ù†ÙÙ‚Ø©": [
                    "ÙƒÙŠÙ ØªÙØ­Ø³Ø¨ Ø§Ù„Ù†ÙÙ‚Ø©ØŸ",
                    "Ù…Ø§ Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù†ÙÙ‚Ø© Ø§Ù„ÙˆØ§Ø¬Ø¨Ø©ØŸ",
                    "Ù…ØªÙ‰ ØªØ³Ù‚Ø· Ø§Ù„Ù†ÙÙ‚Ø©ØŸ"
                ]
            },
            LegalDomain.LABOR: {
                "ÙØµÙ„": [
                    "Ù…Ø§ Ø­Ø§Ù„Ø§Øª Ø§Ù„ÙØµÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ØŸ",
                    "Ù…Ø§ ØªØ¹ÙˆÙŠØ¶Ø§Øª Ø§Ù„ÙØµÙ„ Ø§Ù„ØªØ¹Ø³ÙÙŠØŸ",
                    "Ù…Ø§ Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø·Ø¹Ù† ÙÙŠ Ù‚Ø±Ø§Ø± Ø§Ù„ÙØµÙ„ØŸ"
                ],
                "Ø±Ø§ØªØ¨": [
                    "Ù…Ø§ Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ø¹Ø§Ù…Ù„ ÙÙŠ Ø§Ù„Ø£Ø¬Ø±ØŸ",
                    "Ù…ØªÙ‰ ÙŠØ¬ÙˆØ² Ø§Ù„Ø®ØµÙ… Ù…Ù† Ø§Ù„Ø±Ø§ØªØ¨ØŸ",
                    "ÙƒÙŠÙ ØªÙØ­Ø³Ø¨ Ø§Ù„Ù…Ø³ØªØ­Ù‚Ø§Øª Ø§Ù„Ù…ØªØ£Ø®Ø±Ø©ØŸ"
                ]
            },
            LegalDomain.CRIMINAL: {
                "Ø¹Ù‚ÙˆØ¨Ø©": [
                    "Ù…Ø§ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¹Ù‚ÙˆØ¨Ø§Øª ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø¬Ø²Ø§Ø¦ÙŠØŸ",
                    "Ù…Ø§ Ø§Ù„Ø¸Ø±ÙˆÙ Ø§Ù„Ù…Ø®ÙÙØ© ÙˆØ§Ù„Ù…Ø´Ø¯Ø¯Ø©ØŸ",
                    "Ù…Ø§ Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø§Ø³ØªØ¦Ù†Ø§ÙØŸ"
                ]
            }
        }
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ø³Ø¦Ù„Ø© Ù…Ù†Ø§Ø³Ø¨Ø©
        if domain in domain_questions:
            for keyword, questions in domain_questions[domain].items():
                if keyword in question:
                    sub_questions.extend(questions[:2])
                    break
        
        # Ø£Ø³Ø¦Ù„Ø© Ø­Ø³Ø¨ Ø§Ù„Ù†ÙŠØ©
        intent_questions = {
            "procedure": ["Ù…Ø§ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŸ", "Ù…Ø§ Ø§Ù„Ù…Ø¯Ø© Ø§Ù„Ø²Ù…Ù†ÙŠØ© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©ØŸ"],
            "eligibility": ["Ù…Ø§ Ø§Ù„Ø´Ø±ÙˆØ· Ø§Ù„ÙˆØ§Ø¬Ø¨ ØªÙˆÙØ±Ù‡Ø§ØŸ", "Ù…Ø§ Ø§Ù„Ù…ÙˆØ§Ù†Ø¹ØŸ"],
            "consequence": ["Ù…Ø§ Ø§Ù„Ø¹Ù‚ÙˆØ¨Ø§Øª Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©ØŸ", "Ù…Ø§ Ø³Ø¨Ù„ Ø§Ù„ØªØ®ÙÙŠÙØŸ"],
            "deadline": ["Ù…Ø§ Ø¢Ø®Ø± Ù…ÙˆØ¹Ø¯ØŸ", "Ù…Ø§ ÙŠØªØ±ØªØ¨ Ø¹Ù„Ù‰ ÙÙˆØ§Øª Ø§Ù„Ù…Ø¯Ø©ØŸ"]
        }
        
        if intent in intent_questions:
            sub_questions.extend(intent_questions[intent][:1])
        
        return sub_questions[:5]
    
    def _generate_search_queries(
        self,
        legal_keywords: List[KeywordMatch],
        domain: LegalDomain,
        intent: str
    ) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¨Ø­Ø« Ù…Ø­Ø³Ù‘Ù†Ø©"""
        queries = []
        
        # Ø§Ø³ØªØ¹Ù„Ø§Ù… 1: Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
        main_keywords = [m.keyword for m in legal_keywords[:3]]
        if main_keywords:
            queries.append(" ".join(main_keywords))
        
        # Ø§Ø³ØªØ¹Ù„Ø§Ù… 2: Ø§Ù„Ù…Ø¬Ø§Ù„ + Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹
        if domain != LegalDomain.UNKNOWN and main_keywords:
            queries.append(f"{domain.arabic_name} {main_keywords[0]}")
        
        # Ø§Ø³ØªØ¹Ù„Ø§Ù… 3: Ù…Ø¹ Ø³ÙŠØ§Ù‚ Ø§Ù„Ù†ÙŠØ©
        intent_prefixes = {
            "procedure": "Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª",
            "eligibility": "Ø´Ø±ÙˆØ·",
            "consequence": "Ø¹Ù‚ÙˆØ¨Ø©",
            "deadline": "Ù…Ø¯Ø©"
        }
        
        if intent in intent_prefixes and main_keywords:
            queries.append(f"{intent_prefixes[intent]} {main_keywords[0]}")
        
        # Ø§Ø³ØªØ¹Ù„Ø§Ù… 4: ØªÙˆØ³ÙŠØ¹ Ø¨Ø§Ù„Ù…ØªØ±Ø§Ø¯ÙØ§Øª
        if len(legal_keywords) > 1:
            synonyms = self.fuzzy_matcher.expand_keywords([legal_keywords[0].keyword])
            if len(synonyms) > 1:
                queries.append(" ".join(synonyms[:3]))
        
        return queries[:5]


# ============================================================
# ADVANCED PROMPT BUILDER - Ø¨Ù†Ù‘Ø§Ø¡ Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
# ============================================================

class AdvancedPromptBuilder:
    """Ø¨Ù†Ù‘Ø§Ø¡ Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
    
    def __init__(self):
        self.query_generator = AdvancedQueryGenerator()
        self.magic_words = MagicWordsManager()
        self.templates = PromptTemplateManager()
        self.few_shot = FewShotManager()
        
        logger.info("ğŸ”§ AdvancedPromptBuilder initialized")
    
    def build_analysis_prompt(self, question: str) -> str:
        """Ø¨Ù†Ø§Ø¡ Ø¨Ø±ÙˆÙ…Ø¨Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„"""
        template = self.templates.get_template("question_analyzer", use_ab_testing=True)
        
        magic = "\n".join(self.magic_words.get_words([
            MagicWordCategory.STEP_BY_STEP,
            MagicWordCategory.STRUCTURED_OUTPUT
        ]))
        
        return template.render(
            question=question,
            magic_words=magic
        )
    
    def build_reasoning_prompt(
        self,
        question: str,
        context: str,
        domain: Optional[LegalDomain] = None,
        complexity: Optional[QuestionComplexity] = None,
        include_examples: bool = True
    ) -> str:
        """Ø¨Ù†Ø§Ø¡ Ø¨Ø±ÙˆÙ…Ø¨Øª Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ"""
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙØ­Ø¯Ø¯ Ø§Ù„Ù…Ø¬Ø§Ù„
        if domain is None:
            analysis = self.query_generator.analyze(question)
            domain = analysis.domain
            complexity = analysis.complexity
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù‚Ø§Ù„Ø¨
        template = self.templates.get_template("legal_reasoning", use_ab_testing=True)
        
        # Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø³Ø­Ø±ÙŠØ© Ø­Ø³Ø¨ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
        magic = "\n".join(self.magic_words.get_context_words(
            complexity or QuestionComplexity.MODERATE,
            domain
        ))
        
        # Ø¯ÙˆØ± Ø§Ù„Ø®Ø¨ÙŠØ±
        expert_role = f"Ø£Ù†Øª Ù…Ø­Ø§Ù…Ù Ø³Ø¹ÙˆØ¯ÙŠ Ø®Ø¨ÙŠØ± Ø¨Ø®Ø¨Ø±Ø© 20 Ø¹Ø§Ù…Ø§Ù‹ ÙÙŠ {domain.arabic_name}."
        
        # Ø§Ù„Ø£Ù…Ø«Ù„Ø©
        examples_text = ""
        if include_examples:
            examples = self.few_shot.get_examples(domain)
            examples_text = self.few_shot.format_examples(examples)
        
        return template.render(
            expert_role=expert_role,
            magic_words=magic,
            question=question,
            context=context[:3000],
            few_shot_examples=examples_text,
            domain=domain.arabic_name
        )
    
    def build_verification_prompt(
        self,
        question: str,
        answer: str,
        sources_count: int
    ) -> str:
        """Ø¨Ù†Ø§Ø¡ Ø¨Ø±ÙˆÙ…Ø¨Øª Ø§Ù„ØªØ­Ù‚Ù‚"""
        template = self.templates.get_template("verification")
        
        return template.render(
            question=question,
            answer=answer[:2500],
            sources_count=sources_count
        )
    
    def build_consistency_prompt(
        self,
        question: str,
        context: str,
        perspective: str,
        focus_area: str
    ) -> str:
        """Ø¨Ù†Ø§Ø¡ Ø¨Ø±ÙˆÙ…Ø¨Øª Ø§Ù„ØªÙ†Ø§Ø³Ù‚"""
        magic = "\n".join(self.magic_words.get_words([
            MagicWordCategory.DEEP_THINKING,
            MagicWordCategory.VERIFICATION
        ]))
        
        return f"""Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ù‚Ø§Ù†ÙˆÙ†ÙŠ. ÙÙƒØ± ÙÙŠ Ø§Ù„Ù…Ø³Ø£Ù„Ø© Ù…Ù† Ù…Ù†Ø¸ÙˆØ± {perspective}.

{magic}

**Ø§Ù„Ø³Ø¤Ø§Ù„:** {question}
**Ø§Ù„Ø³ÙŠØ§Ù‚:** {context[:1500]}

**Ù…Ù† ÙˆØ¬Ù‡Ø© Ù†Ø¸Ø± {perspective}:**
Ø±ÙƒØ² Ø¹Ù„Ù‰ {focus_area} ÙˆÙ‚Ø¯Ù… ØªØ­Ù„ÙŠÙ„Ùƒ.

**Ø§Ù„ØªØ­Ù„ÙŠÙ„:**"""
    
    def get_magic_words(
        self,
        categories: List[MagicWordCategory],
        domain: Optional[LegalDomain] = None
    ) -> List[str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ø³Ø­Ø±ÙŠØ©"""
        return self.magic_words.get_words(categories, domain)
    
    def record_prompt_performance(
        self,
        template_id: str,
        success: bool,
        quality: float,
        response_time: float
    ):
            self.templates.record_performance(template_id, success, quality, response_time)


# ============================================================================
# Backward Compatibility Aliases (Ù„Ù„ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù‚Ø¯ÙŠÙ…)
# ============================================================================

# Alias Ù„Ù„ÙƒÙ„Ø§Ø³Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
IntelligentQueryGenerator = AdvancedQueryGenerator
PromptBuilder = AdvancedPromptBuilder

# Ø¥Ø¶Ø§ÙØ© method alias Ù„Ù„ØªÙˆØ§ÙÙ‚
def _add_analyze_question_alias():
    """Ø¥Ø¶Ø§ÙØ© analyze_question ÙƒÙ€alias Ù„Ù€analyze"""
    AdvancedQueryGenerator.analyze_question = AdvancedQueryGenerator.analyze

_add_analyze_question_alias()

# Backward compatibility constants
MAGIC_WORDS = {
    "greeting": ["Ù…Ø±Ø­Ø¨Ø§", "Ø§Ù„Ø³Ù„Ø§Ù…", "Ø§Ù‡Ù„Ø§", "ØµØ¨Ø§Ø­", "Ù…Ø³Ø§Ø¡"],
    "thanks": ["Ø´ÙƒØ±Ø§", "Ù…ØªØ´ÙƒØ±"],
    "question": ["ÙƒÙŠÙ", "Ù…Ø§Ø°Ø§", "Ù‡Ù„", "Ù…ØªÙ‰", "Ø£ÙŠÙ†"]
}

LEGAL_DOMAIN_KEYWORDS = {
    "Ø£Ø­ÙˆØ§Ù„ Ø´Ø®ØµÙŠØ©": ["Ø·Ù„Ø§Ù‚", "Ø²ÙˆØ§Ø¬", "Ø­Ø¶Ø§Ù†Ø©", "Ù†ÙÙ‚Ø©"],
    "ØªØ¬Ø§Ø±ÙŠ": ["Ø´Ø±ÙƒØ©", "Ø¹Ù‚Ø¯", "ØªØ¬Ø§Ø±Ø©"],
    "Ø¬Ù†Ø§Ø¦ÙŠ": ["Ù‚ØªÙ„", "Ø³Ø±Ù‚Ø©", "ØªØ²ÙˆÙŠØ±"],
    "Ø­Ù‚ÙˆÙ‚": ["Ù…Ù„ÙƒÙŠØ©", "Ø¹Ù‚Ø§Ø±", "ØªØ¹ÙˆÙŠØ¶"]
}

PROMPT_TEMPLATES = {
    "simple": "Ø§Ø¨Ø­Ø« Ø¹Ù†: {query}",
    "detailed": "Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…ÙØµÙ„: {query}"
}


# ============================================================
# EXPORTS
# ============================================================

__all__ = [
    # Enums
    "LegalDomain",
    "QuestionComplexity",
    "PromptStyle",
    "MagicWordCategory",
    
    # Data structures
    "AnalyzedQuestion",
    "KeywordMatch",
    "DomainScore",
    "PromptTemplate",
    "FewShotExample",
    
    # Managers
    "MagicWordsManager",
    "FuzzyKeywordMatcher",
    "SemanticDomainDetector",
    "ComplexityAnalyzer",
    "IntentDetector",
    "EntityExtractor",
    "FewShotManager",
    "PromptTemplateManager",
    
    # Main classes
    "AdvancedQueryGenerator",
    "AdvancedPromptBuilder",
    
    # Backward compatibility
    "IntelligentQueryGenerator",  # Alias
    "PromptBuilder",  # Alias
    "MAGIC_WORDS",
    "LEGAL_DOMAIN_KEYWORDS",
    "PROMPT_TEMPLATES"
]