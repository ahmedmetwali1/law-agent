"""
Enhanced General Lawyer Agent
Ø§Ù„ÙˆÙƒÙŠÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ø¹Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù† - Ù†Ø³Ø®Ø© Ø¹Ø¨Ù‚Ø±ÙŠØ©

Integrates all advanced features:
- Multi-Tiered Memory System
- Hybrid Reasoning Engine (CoT + ReAct)
- Self-Regulated Retrieval
- DEPART Framework
- Confidence Tracking

Based on 2025-2026 research papers
"""

from typing import Dict, Any, List, Optional, Callable
import logging
import json
from datetime import datetime

from .base_agent import LLMAgent
from ..memory import (
    MultiTieredMemory,
    MemoryItem,
    MemoryImportance,
    MemoryConsolidator
)
from ..reasoning import (
    HybridReasoningEngine,
    ReasoningMode,
    QueryComplexity
)
from ..retrieval import (
    SelfRegulatedRetrieval,
    RetrievalContext,
    ThinkingSpeed
)
from ..reasoning.depart_engine import DEPARTEngine
from .enhanced_conduct_intake import conduct_intake_with_tools

logger = logging.getLogger(__name__)


class EnhancedGeneralLawyerAgent(LLMAgent):
    """
    Enhanced General Lawyer Agent with genius-level capabilities
    
    Features:
    - ðŸ§  Multi-tiered memory (working, episodic, long-term, meta)
    - ðŸ”€ Hybrid reasoning (CoT + ReAct + Neural-Symbolic)
    - âš¡ Self-regulated retrieval (fast / slow thinking)
    - ðŸ“Š Confidence tracking for every decision
    - ðŸ”„ Automatic memory consolidation
    - ðŸŽ¯ DEPART framework for complex tasks
    """
    
    def __init__(
        self,
        lawyer_id: Optional[str] = None,
        lawyer_name: Optional[str] = None,
        current_user: Optional[Dict[str, Any]] = None,  # NEW: The user actually chatting (could be assistant)
        use_multi_tiered_memory: bool = True,
        reasoning_mode: str = "hybrid",  # "cot", "react", "hybrid", "auto"
        self_regulated_retrieval: bool = True,
        enable_consolidation: bool = True
    ):
        """
        Initialize Enhanced General Lawyer Agent as Office Manager
        
        Args:
            lawyer_id: Lawyer's user ID
            lawyer_name: Lawyer's full name
            use_multi_tiered_memory: Enable multi-tiered memory
            reasoning_mode: hybrid, cot, react, tot
            self_regulated_retrieval: Enable self-regulated RAG
            enable_consolidation: Enable auto-consolidation
        """
        # Set lawyer context BEFORE super().__init__()
        self.lawyer_id = lawyer_id
        self.lawyer_name = lawyer_name or "Ø§Ù„Ù…Ø­Ø§Ù…ÙŠ"
        self.current_user = current_user
        
        # Determine if the charter is an assistant
        self.is_assistant = False
        if self.current_user and self.lawyer_id:
            chat_user_id = str(self.current_user.get("id", ""))
            if chat_user_id and chat_user_id != str(self.lawyer_id):
                self.is_assistant = True
                logger.info(f"ðŸ‘¤ Agent initialized in ASSISTANT mode. Caller: {self.current_user.get('full_name')} -> Office: {self.lawyer_name}")
        
        # Lawyer detailed information (will be injected in system prompt)
        self.lawyer_info = {
            "id": lawyer_id,
            "full_name": lawyer_name or "Ø§Ù„Ù…Ø­Ø§Ù…ÙŠ",
            "email": None,
            "phone": None,
            "specialization": None,
            "license_number": None,
            "office_location": None
        }
        
        # Load lawyer detailed info if lawyer_id is provided
        if lawyer_id:
            self._load_lawyer_info()
        
        # Initialize base LLMAgent as Office Manager
        super().__init__(
            name="Ù…Ø¯ÙŠØ± Ø§Ù„Ù…ÙƒØªØ¨ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ",
            role="Ø§Ù„Ø°Ø±Ø§Ø¹ Ø§Ù„Ø£ÙŠÙ…Ù† Ø§Ù„Ø±Ù‚Ù…ÙŠ Ù„Ù„Ù…Ø­Ø§Ù…ÙŠ - Ù…Ø¯ÙŠØ± Ù…ÙƒØªØ¨ Ø°ÙƒÙŠ",
            expertise="Ø¥Ø¯Ø§Ø±Ø© Ø´Ø§Ù…Ù„Ø© Ù„Ù…ÙƒØªØ¨ Ø§Ù„Ù…Ø­Ø§Ù…Ø§Ø©: Ø§Ù„Ù…ÙˆÙƒÙ„ÙŠÙ†, Ø§Ù„Ù‚Ø¶Ø§ÙŠØ§, Ø§Ù„Ø¬Ù„Ø³Ø§Øª, ÙˆØ§Ù„Ù…Ù‡Ø§Ù…"
        )
        
        # Personal Assistant Tools (will be initialized when lawyer_id is set)
        self.client_tools = None
        self.case_tools = None
        self.hearing_tools = None
        self.profile_tools = None
        self.unified_tools = None
        
        # Plan Tracker Tool (for step-by-step execution tracking)
        from ..tools.plan_tracker_tool import PlanTrackerTool
        self.plan_tracker = PlanTrackerTool()
        logger.info("ðŸ“‹ Plan Tracker Tool initialized")
        
        if self.lawyer_id:
            self._initialize_tools()
        
        # Advanced memory system
        if use_multi_tiered_memory:
            logger.info("ðŸ§  Initializing Multi-Tiered Memory...")
            self.memory_system = MultiTieredMemory(working_capacity=25)
            
            if enable_consolidation:
                self.memory_consolidator = MemoryConsolidator(self.memory_system)
            else:
                self.memory_consolidator = None
        else:
            self.memory_system = None
            self.memory_consolidator = None
        
        # Hybrid reasoning engine
        logger.info("ðŸ”€ Initializing Hybrid Reasoning Engine...")
        self.reasoning_engine = HybridReasoningEngine(
            llm_agent=self,
            tools=[]  # Will be populated later
        )
        
        # Deontic Logic System - ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ§Ø¬Ø¨Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
        logger.info("âš–ï¸ Initializing Deontic Logic System...")
        from ..reasoning import DeonticLogicSystem
        self.deontic_logic = DeonticLogicSystem()
        
        # Temporal Logic System - ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯ ÙˆØ§Ù„Ù…Ù‡Ù„
        logger.info("ðŸ“… Initializing Temporal Logic System...")
        from ..reasoning import TemporalLogicSystem
        self.temporal_logic = TemporalLogicSystem()
        
        # Advanced Thinking Loop - Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª ØªÙÙƒÙŠØ± Ù…ØªÙ‚Ø¯Ù…Ø© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
        try:
            logger.info("ðŸ§  Initializing AdvancedThinkingLoop...")
            from ..reasoning.thinking_loop import AdvancedThinkingLoop
            from ..config.openwebui import openwebui_client
            
            # Pass required dependencies to fix initialization error
            self.advanced_thinking = AdvancedThinkingLoop(
                llm_client=openwebui_client,
                search_engine=None,  # Will be set later if needed
                deontic_system=self.deontic_logic,
                temporal_system=self.temporal_logic,
                confidence_calculator=None,  # Will create internally
                cache=None,  # Optional - will create if needed
                should_cache=True,
                use_deontic=True,
                use_temporal=True,
                use_counterfactuals=False,  # Disabled by default for performance
                use_neural_symbolic=False  # Disabled by default for performance
            )
            self.use_advanced_thinking = True
            logger.info("âœ… AdvancedThinkingLoop initialized")
        except Exception as e:
            logger.warning(f"âš ï¸ AdvancedThinkingLoop not available: {e}")
            self.advanced_thinking = None
            self.use_advanced_thinking = False
        
        # Self-regulated retrieval
        if self_regulated_retrieval:
            logger.info("âš¡ Initializing Self-Regulated Retrieval...")
            self.retrieval_system = SelfRegulatedRetrieval(
                memory_system=self.memory_system
            )
        else:
            self.retrieval_system = None
        
        # DEPART engine for complex tasks
        logger.info("ðŸŽ¯ Initializing DEPART Engine...")
        self.depart = DEPARTEngine(agent=self)
        
        # Configuration
        self.config = {
            "reasoning_mode": reasoning_mode,
            "use_memory": use_multi_tiered_memory,
            "use_self_regulation": self_regulated_retrieval,
            "enable_consolidation": enable_consolidation,
            "use_deontic_logic": True,  # Always enabled
            "use_temporal_logic": True,  # Always enabled
            "use_advanced_thinking": self.use_advanced_thinking
        }
        
        # Track session
        self.current_session = {
            "start_time": datetime.now(),
            "interactions": 0,
            "memory_consolidations": 0
        }
        
        logger.info("=" * 60)
        logger.info("âœ… Enhanced General Lawyer Agent initialized successfully!")
        logger.info(f"   Memory System: {'âœ“' if use_multi_tiered_memory else 'âœ—'}")
        logger.info(f"   Reasoning Mode: {reasoning_mode}")
        logger.info(f"   Self-Regulation: {'âœ“' if self_regulated_retrieval else 'âœ—'}")
        logger.info(f"   Auto-Consolidation: {'âœ“' if enable_consolidation else 'âœ—'}")
        logger.info(f"   Deontic Logic: âœ“")
        logger.info(f"   Temporal Logic: âœ“")
        logger.info(f"   Advanced Thinking: {'âœ“' if self.use_advanced_thinking else 'âœ—'}")
        logger.info("=" * 60)
    
    def set_lawyer_context(self, lawyer_id: str, lawyer_name: str):
        """
        Set lawyer context and initialize tools
        
        Args:
            lawyer_id: Lawyer's user ID
            lawyer_name: Lawyer's full name
        """
        self.lawyer_id = lawyer_id
        self.lawyer_name = lawyer_name
        
        # Load detailed info first
        self._load_lawyer_info()
        
        # Initialize tools
        self._initialize_tools()
        
        # CRITICAL: Update system prompt with loaded info
        self.system_prompt = self._default_system_prompt()
        
        logger.info(f"âœ… Lawyer context set: {self.lawyer_name} ({lawyer_id})")
    
    def _load_lawyer_info(self):
        """Load detailed lawyer information from database"""
        if not self.lawyer_id:
            logger.warning("âš ï¸ Cannot load lawyer info without lawyer_id")
            return
        
        try:
            from agents.storage.user_storage import user_storage
            
            logger.info(f"ðŸ“¥ Loading lawyer info for ID: {self.lawyer_id}")
            user = user_storage.get_user_by_id(self.lawyer_id)
            
            if user:
                self.lawyer_info.update({
                    "id": user.get("id"),
                    "full_name": user.get("full_name", self.lawyer_name),
                    "email": user.get("email"),
                    "phone": user.get("phone"),
                    "specialization": user.get("specialization"),
                    "license_number": user.get("license_number"),
                    "office_location": f"{user.get('office_city', '')} - {user.get('office_address', '')}".strip(" -")
                })
                self.lawyer_name = self.lawyer_info["full_name"]
                logger.info(f"âœ… Lawyer info loaded: {self.lawyer_info['full_name']}")
            else:
                logger.warning(f"âš ï¸ No lawyer found with ID: {self.lawyer_id}")
        except Exception as e:
            logger.error(f"âŒ Failed to load lawyer info: {e}")
    
    def _initialize_tools(self):
        """Initialize all personal assistant tools"""
        if not self.lawyer_id:
            logger.warning("âš ï¸ Cannot initialize tools without lawyer_id")
            return
        
        # Initialize Unified Tool System
        from agents.tools.unified_tools import UnifiedToolSystem
        
        logger.info(f"ðŸ”§ Initializing Unified Tool System for {self.lawyer_name}...")
        self.unified_tools = UnifiedToolSystem(
            lawyer_id=self.lawyer_id,
            lawyer_name=self.lawyer_name,
            current_user=self.current_user
        )
        logger.info(f"âœ… Unified Tools ready with {len(self.unified_tools.get_available_tools_list())} tools")
        

        
        logger.info("âœ… All assistant tools initialized")

    def generate_response(
        self,
        messages: List[Dict[str, str]] = None,
        temperature: float = None,
        max_tokens: int = None
    ) -> str:
        """
        Generate response with automatic tool execution
        """
        # Get tools if available
        tools = None
        if hasattr(self, 'unified_tools') and self.unified_tools:
            tools = self.unified_tools.get_tools_for_llm()
            
        # Check for complex tasks based on content analysis
        # Using simple heuristic or specialized prompt
        user_msg = messages[-1]["content"] if messages else ""
        is_complex = len(user_msg.split()) > 10 and any(keyword in user_msg for keyword in ["Ø­Ù„Ù„", "Ø®Ø·Ø©", "Ø±Ø£ÙŠÙƒ", "Ø¯Ø±Ø§Ø³Ø©", "ØªÙ‚ÙŠÙŠÙ…"])
        
        if is_complex and not tools:
             # Complex task without specific tools requested -> use planning
             logger.info("ðŸ§  Detailed planning triggered for complex task")
             
             if getattr(self, 'current_thought_callback', None):
                 self.current_thought_callback("Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø·Ù„Ø¨ ÙˆØ¥Ù†Ø´Ø§Ø¡ Ø®Ø·Ø© ØªÙØµÙŠÙ„ÙŠØ©...")
             
             # Extract case facts from message history
             case_facts = "\n".join([m["content"] for m in messages if m["role"] == "user"])
             
             # Execute plan
             result = self.create_and_execute_plan(
                 case_facts=case_facts,
                 case_id=self.plan_tracker.current_plan.plan_id if self.plan_tracker.current_plan else None
             )
             
             return result.get("final_recommendation", "ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„")

        # Call LLM
        response = super().generate_response(
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            tools=tools
        )
        
        # Check for tool calls (OpenAI/OpenWebUI format)
        if hasattr(response, 'tool_calls') and response.tool_calls:
            tool_calls = response.tool_calls
            logger.info(f"ðŸ› ï¸ LLM requested {len(tool_calls)} tool calls")
            
            # Add assistant message with tool calls to memory
            self.add_message(
                role="assistant",
                content=response.content,
                metadata={"tool_calls": tool_calls}
            )
            
            # Execute each tool
            for tool_call in tool_calls:
                function_name = tool_call.function.name
                arguments = tool_call.function.arguments
                tool_call_id = tool_call.id
                
                try:
                    # Parse arguments
                    function_args = json.loads(arguments)
                    
                    if getattr(self, 'current_thought_callback', None):
                        self.current_thought_callback(f"Ø¬Ø§Ø±ÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø¯Ø§Ø©: {function_name}...")

                    # Execute
                    result = self.unified_tools.execute_tool(
                        function_name,
                        **function_args
                    )
                    
                    if getattr(self, 'current_thought_callback', None):
                         status = "Ù†Ø¬Ø§Ø­" if "error" not in str(result).lower() else "ÙØ´Ù„"
                         self.current_thought_callback(f"ØªÙ… ØªÙ†ÙÙŠØ° {function_name}: {status}")
                    
                    # Add tool result to memory
                    self.add_message(
                        role="tool",
                        content=json.dumps(result, ensure_ascii=False),
                        metadata={"tool_call_id": tool_call_id}
                    )
                    
                except Exception as e:
                    logger.error(f"âŒ Error executing tool {function_name}: {e}")
                    self.add_message(
                        role="tool",
                        content=json.dumps({"error": str(e)}),
                        metadata={"tool_call_id": tool_call_id}
                    )
            
            # Recurse to get final response (with new history)
            return self.generate_response(messages=None)
            
        # Handle simple content response
        content = ""
        if hasattr(response, 'content'):
            content = response.content
        elif isinstance(response, str):
            content = response
            
        return content or ""
    
    def _default_system_prompt(self) -> str:
        """Get system prompt with injected lawyer information"""
        
        # Format lawyer info for injection
        lawyer_profile = f"""
## ðŸ‘¤ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø­Ø§Ù…ÙŠ (ØµØ§Ø­Ø¨ Ø§Ù„Ù…ÙƒØªØ¨)

**Ø§Ù„Ø§Ø³Ù… Ø§Ù„ÙƒØ§Ù…Ù„**: {self.lawyer_info.get('full_name', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}
**Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ**: {self.lawyer_info.get('email', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}
**Ø±Ù‚Ù… Ø§Ù„Ù‡Ø§ØªÙ**: {self.lawyer_info.get('phone', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}
**Ø§Ù„ØªØ®ØµØµ**: {self.lawyer_info.get('specialization', 'Ù…Ø­Ø§Ù…Ø§Ø© Ø¹Ø§Ù…Ø©')}
**Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…ÙƒØªØ¨**: {self.lawyer_info.get('office_location', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}
"""

        # Current User Context (Who am I talking to?)
        user_context_section = ""
        if self.is_assistant and self.current_user:
            user_context_section = f"""
## ðŸ—£ï¸ Ø£Ù†Øª ØªØªØ­Ø¯Ø« Ù…Ø¹ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯:
**Ø§Ù„Ø§Ø³Ù…**: {self.current_user.get('full_name', 'Ù…Ø³Ø§Ø¹Ø¯')}
**Ø§Ù„Ø¯ÙˆØ±**: {self.current_user.get('role', 'Ù…Ø³Ø§Ø¹Ø¯')}
**ØªÙ†Ø¨ÙŠÙ‡ Ù‡Ø§Ù…**: Ù‡Ø°Ø§ Ù„ÙŠØ³ Ø§Ù„Ù…Ø­Ø§Ù…ÙŠØŒ Ø¨Ù„ Ù…Ø³Ø§Ø¹Ø¯ ÙŠØ¹Ù…Ù„ ÙÙŠ Ù…ÙƒØªØ¨Ù‡.
"""
        else:
             user_context_section = f"""
## ðŸ—£ï¸ Ø£Ù†Øª ØªØªØ­Ø¯Ø« Ù…Ø¹ Ø§Ù„Ù…Ø­Ø§Ù…ÙŠ (Ù…Ø§Ù„Ùƒ Ø§Ù„Ù…ÙƒØªØ¨) Ù…Ø¨Ø§Ø´Ø±Ø©:
**Ø§Ù„Ø§Ø³Ù…**: {self.lawyer_info.get('full_name')}
"""
        
        role_instruction = ""
        if self.is_assistant:
            role_instruction = f"""
### âš ï¸ ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø®Ø§ØµØ© Ø¨Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ÙŠÙ†:
1. Ø£Ù†Øª ØªØªØ­Ø¯Ø« Ù…Ø¹ **{self.current_user.get('full_name')}** (Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯).
2. Ù„ÙƒÙ†Ùƒ ØªØ¹Ù…Ù„ Ù„ØµØ§Ù„Ø­ **Ø§Ù„Ù…Ø­Ø§Ù…ÙŠ {self.lawyer_info.get('full_name')}** (Ù…Ø¯ÙŠØ± Ø§Ù„Ù…ÙƒØªØ¨).
3. **Ù…Ù‡Ù… Ø¬Ø¯Ø§Ù‹**: Ø¹Ù†Ø¯ Ø¥Ù†Ø´Ø§Ø¡ Ø£ÙŠ Ø¨ÙŠØ§Ù†Ø§Øª (Ù‚Ø¶Ø§ÙŠØ§ØŒ Ù…ÙˆÙƒÙ„ÙŠÙ†ØŒ Ù…Ù‡Ø§Ù…)ØŒ ÙŠØ¬Ø¨ Ø£Ù† ØªØ³ØªØ®Ø¯Ù… Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ø­Ø§Ù…ÙŠ `lawyer_id` = `{self.lawyer_id}` Ù„Ø¶Ù…Ø§Ù† ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ø³Ù… Ø§Ù„Ù…ÙƒØªØ¨ØŒ ÙˆÙ„ÙŠØ³ Ø¨Ø§Ø³Ù… Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯.
4. Ø®Ø§Ø·Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ù„Ù‚Ø¨ "Ø£Ø³ØªØ§Ø°/Ø© {self.current_user.get('full_name')}".
"""
        else:
            role_instruction = f"""
### ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ø­Ø§Ù…ÙŠ:
1. Ø£Ù†Øª ØªØªØ­Ø¯Ø« Ù…Ø¨Ø§Ø´Ø±Ø© Ù…Ø¹ Ù…Ø§Ù„Ùƒ Ø§Ù„Ù…ÙƒØªØ¨.
2. Ù†ÙØ° Ø£ÙˆØ§Ù…Ø±Ù‡ ÙÙˆØ±Ø§Ù‹.
3. Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØªØ³Ø¬Ù„ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¨Ø§Ø³Ù…Ù‡.
"""

        return f"""# Ø£Ù†Øª Ù…Ø¯ÙŠØ± Ø§Ù„Ù…ÙƒØªØ¨ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ Ø§Ù„Ø°ÙƒÙŠ (The Intelligent Digital Manager) ðŸ§ 

{lawyer_profile}

{user_context_section}

{role_instruction}

## ðŸŽ¯ Ø§Ù„Ù‡ÙˆÙŠØ© ÙˆØ§Ù„Ø±Ø¤ÙŠØ©
Ø£Ù†Øª Ù„Ø³Øª Ù…Ø¬Ø±Ø¯ "Ø¨ÙˆØª Ù…Ø­Ø§Ø¯Ø«Ø©"ØŒ Ø£Ù†Øª **Ø´Ø±ÙŠÙƒ Ø°ÙƒÙŠ** ÙÙŠ Ø¥Ø¯Ø§Ø±Ø© Ù…ÙƒØªØ¨ Ø§Ù„Ù…Ø­Ø§Ù…Ø§Ø©.
Ù…Ù‡Ù…ØªÙƒ Ù„ÙŠØ³Øª ÙÙ‚Ø· ØªÙ†ÙÙŠØ° Ø§Ù„Ø£ÙˆØ§Ù…Ø±ØŒ Ø¨Ù„ **ÙÙ‡Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ ÙˆØ§Ù„ØªÙÙƒÙŠØ±ØŒ Ø«Ù… Ø§Ù„ØªÙ†ÙÙŠØ° Ø¨Ø¯Ù‚Ø©**.
Ø£Ù†Øª Ø­Ø§Ø±Ø³ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙ…ØµØ¯Ø± Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø© ÙÙŠ Ø§Ù„Ù…ÙƒØªØ¨.

âš ï¸ **ØªØ¹Ù„ÙŠÙ…Ø§Øª ØµØ§Ø±Ù…Ø© (CRITICAL)**:
Ø¹Ù†Ø¯Ù…Ø§ ØªÙ‚ÙˆÙ… Ø¨Ø§Ù„ØªÙÙƒÙŠØ± Ø£Ùˆ Ø§Ù„ØªØ­Ù„ÙŠÙ„ (Reasoning)ØŒ **ÙŠØ¬Ø¨** Ø£Ù† ØªÙ†ØªÙ‡ÙŠ Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø¨Ø¥Ø¬Ø±Ø§Ø¡ Ù…Ù„Ù…ÙˆØ³ (Tool Call) Ø£Ùˆ Ø¥Ø¬Ø§Ø¨Ø© Ù†Ù‡Ø§Ø¦ÙŠØ© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù….
**Ù…Ù…Ù†ÙˆØ¹ Ø§Ù„ØªÙˆÙ‚Ù Ø¨Ø¹Ø¯ Ø§Ù„ØªÙÙƒÙŠØ± ÙÙ‚Ø·.**
Ø¥Ø°Ø§ Ù‚Ø±Ø±Øª Ø£Ù†Ùƒ Ø¨Ø­Ø§Ø¬Ø© Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø¯Ø§Ø©ØŒ Ù‚Ù… Ø¨Ø§Ø³ØªØ¯Ø¹Ø§Ø¦Ù‡Ø§ ÙÙˆØ±Ø§Ù‹.

---

---

## âš™ï¸ Ù…Ù†Ù‡Ø¬ÙŠØ© Ø§Ù„Ø¹Ù…Ù„ (Think-Act-Report)

### 1. ðŸ§  Ø§Ù„ØªÙÙƒÙŠØ± Ø£ÙˆÙ„Ø§Ù‹ (Chain of Thought)
Ù‚Ø¨Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙŠ Ø£Ø¯Ø§Ø©ØŒ ÙÙƒØ± Ù…Ø¹ Ù†ÙØ³Ùƒ (Ø¯Ø§Ø®Ù„ÙŠØ§Ù‹):
- **Ù…Ø§Ø°Ø§ ÙŠØ±ÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø­Ù‚Ø§Ù‹ØŸ** (Ù‡Ù„ ÙŠØ±ÙŠØ¯ Ø¥Ø¶Ø§ÙØ© Ù‚Ø¶ÙŠØ© Ø¬Ø¯ÙŠØ¯Ø© Ø£Ù… Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø¹Ù„Ù‰ ÙˆØ§Ø­Ø¯Ø© Ø­Ø§Ù„ÙŠØ©ØŸ)
- **Ù‡Ù„ Ù„Ø¯ÙŠ ÙƒÙ„ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§ØªØŸ** (Ø¥Ø°Ø§ Ø·Ù„Ø¨ "Ø¥Ø¶Ø§ÙØ© Ø¬Ù„Ø³Ø©"ØŒ Ù‡Ù„ Ø£Ø¹Ø±Ù Ù„Ø£ÙŠ Ù‚Ø¶ÙŠØ©ØŸ)
- **Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø£ÙØ¶Ù„ØŸ** (Ù‡Ù„ Ø£Ø¨Ø­Ø« Ø£ÙˆÙ„Ø§Ù‹ Ù„Ø£Ø¬Ø¯ Ø§Ù„Ù€ IDØŸ)

### 2. ðŸ› ï¸ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø°ÙƒÙŠØ© (Universal Tool Strategy)
Ø£Ù†Øª ØªÙ…ØªÙ„Ùƒ Ø£Ø¯ÙˆØ§Øª Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ© (`insert_`, `update_`, `query_`). Ø§Ø³ØªØ®Ø¯Ù…Ù‡Ø§ Ø¨Ø°ÙƒØ§Ø¡:

- **Ø¹Ù†Ø¯ Ø§Ù„Ø¥Ø¶Ø§ÙØ© (Creation)**:
   - âš ï¸ **ØªØ­Ù‚Ù‚ Ø£ÙˆÙ„Ø§Ù‹**: Ø§Ø¨Ø­Ø« Ø¨Ø§Ù„Ø§Ø³Ù… Ù„ØªØªØ£ÙƒØ¯ Ø£Ù† Ø§Ù„Ø³Ø¬Ù„ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ Ù…Ø³Ø¨Ù‚Ø§Ù‹ (ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±).
   - âœ… **Ù†ÙØ°**: Ø§Ø³ØªØ®Ø¯Ù… `insert_table` Ù…Ø¹ ÙƒØ§ÙØ© Ø§Ù„Ø­Ù‚ÙˆÙ„ Ø§Ù„Ø¥Ø¬Ø¨Ø§Ø±ÙŠØ©.

- **Ø¹Ù†Ø¯ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„/Ø§Ù„ØªØ­Ø¯ÙŠØ« (Update)**:
   - ðŸ” **Ø§Ù„Ø¨Ø­Ø« Ø¥Ù„Ø²Ø§Ù…ÙŠ**: Ù„Ø§ ØªÙ‚Ù… Ø¨Ø§Ù„ØªØ®Ù…ÙŠÙ†. Ø§Ø³ØªØ®Ø¯Ù… `query_logs` Ø£Ùˆ `query_tasks` Ø£ÙˆÙ„Ø§Ù‹ Ù„Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¬Ù„ Ø¨Ø§Ù„Ù…ÙˆØ§ØµÙØ§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©.
   - ðŸ†” **Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ø±Ù**: Ø®Ø° `id` Ù…Ù† Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø¨Ø­Ø«.
   - âœï¸ **Ø§Ù„ØªØ¹Ø¯ÙŠÙ„**: Ø§Ø³ØªØ®Ø¯Ù… `update_table` Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ø§ Ø§Ù„Ù€ `id` Ø­ØµØ±Ø§Ù‹.
   - âš ï¸ **ØªØ­Ø°ÙŠØ±**: Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… `insert_` Ø£Ø¨Ø¯Ø§Ù‹ Ù„ØºØ±Ø¶ Ø§Ù„ØªØ­Ø¯ÙŠØ«.

- **Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø°Ù (Deletion)**:
   - ðŸ” **Ø§Ù„Ø¨Ø­Ø« Ø£ÙˆÙ„Ø§Ù‹**: ØªÙ…Ø§Ù…Ø§Ù‹ Ù…Ø«Ù„ Ø§Ù„ØªØ­Ø¯ÙŠØ«ØŒ Ø§Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù€ `id`.
   - ðŸ›‘ **Ø§Ù„Ø­Ø°Ø±**: Ø§Ø¹Ø±Ø¶ Ø§Ù„ØªÙØ§ØµÙŠÙ„.
   - âš ï¸ **Ø§Ù„ØªØ³Ù„Ø³Ù„ Ø§Ù„Ø¥Ù„Ø²Ø§Ù…ÙŠ (Cascade)**: Ø§Ø­Ø°Ù Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø§Øª Ø£ÙˆÙ„Ø§Ù‹.
   - ðŸ—‘ï¸ **Ø§Ù„ØªÙ†ÙÙŠØ°**: `delete_table` Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù€ `id`.

### 3. ðŸ›¡ï¸ Ø§Ù„Ù†Ø²Ø§Ù‡Ø© ÙˆØ§Ù„Ø´ÙØ§ÙÙŠØ© (Audit & Integrity)
- **Ø³Ø¬Ù„ Ø§Ù„ØªØ¯Ù‚ÙŠÙ‚ (Audit Log)** Ù‡Ùˆ Ù…Ø±Ø¢Ø© Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø©. Ø§Ù„Ù†Ø¸Ø§Ù… ÙŠØ³Ø¬Ù„ ÙƒÙ„ Ø´ÙŠØ¡ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹.
- **Ù…Ù…Ù†ÙˆØ¹ Ø§Ù„ÙƒØ°Ø¨**: Ù„Ø§ ØªØ®ØªÙ„Ù‚ Ù…Ø¹Ø±ÙØ§Øª (UUIDs) Ø£Ùˆ ØªÙˆØ§Ø±ÙŠØ® ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©.
- **Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±**: Ø¹Ù†Ø¯ Ø·Ù„Ø¨ ØªÙ‚Ø±ÙŠØ±ØŒ Ø§Ù‚Ø±Ø£ `audit_logs` ÙˆØªØ±Ø¬Ù… Ø§Ù„Ø­Ø§Ù„Ø§Øª Ù„Ù„Ù‚Ø§Ø±Ø¦ (pending -> Ù‚ÙŠØ¯ Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø±).

---

## ðŸ’¡ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ø§Ù„Ø°ÙƒÙŠ (Context & Intelligence)

- **Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠØ©**: Ø¥Ø°Ø§ ÙƒÙ†Ø§ Ù†ØªØ­Ø¯Ø« Ø¹Ù† "Ù‚Ø¶ÙŠØ© Ø§Ù„ÙˆØ±Ø«"ØŒ ÙˆØ£Ø±Ø¯Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… "Ø£Ø¶Ù Ø¬Ù„Ø³Ø©"ØŒ ÙØ§Ù„Ù…Ù‚ØµÙˆØ¯ Ù‡ÙŠ Ø§Ù„Ù‚Ø¶ÙŠØ© Ø§Ù„Ø­Ø§Ù„ÙŠØ©. Ù„Ø§ ØªØ³Ø£Ù„ Ù…Ø¬Ø¯Ø¯Ø§Ù‹.
- **Ù…Ù„Ø¡ Ø§Ù„ÙØ±Ø§ØºØ§Øª (Proactive)**: Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø© Ù†Ø§Ù‚ØµØ© Ù„ÙƒÙ†Ù‡Ø§ Ø¨Ø¯ÙŠÙ‡ÙŠØ© Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ Ø§Ø³ØªÙ†ØªØ¬Ù‡Ø§. Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø­Ø±Ø¬Ø©ØŒ Ø§Ø³Ø£Ù„ Ø¹Ù†Ù‡Ø§ Ø¨Ø°ÙƒØ§Ø¡.
- **Ø§Ù„Ù„ØºØ©**: Ø±Ø¯ÙˆØ¯Ùƒ Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ Ø§Ù„Ù…Ù‡Ù†ÙŠØ©ØŒ Ù…Ø®ØªØµØ±Ø©ØŒ ÙˆÙˆØ§Ø¶Ø­Ø©.

## ðŸš« Ù…Ø­Ø¸ÙˆØ±Ø§Øª (Strict Constraints)
1. Ù„Ø§ ØªÙ‚Ù… Ø£Ø¨Ø¯Ø§Ù‹ Ø¨ØªØ¹Ø¯ÙŠÙ„ Ø£Ùˆ Ø­Ø°Ù Ø³Ø¬Ù„Ø§Øª `audit_logs` (Ù…Ø­Ø¸ÙˆØ± ØªÙ‚Ù†ÙŠØ§Ù‹ ÙˆØ£Ø®Ù„Ø§Ù‚ÙŠØ§Ù‹).
2. Ù„Ø§ ØªØ¹Ø±Ø¶ Ù…Ø¹Ø±ÙØ§Øª UUIDs Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø› Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ ÙˆØ§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†.
3. Ù„Ø§ ØªÙ‚Ù… Ø¨Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ù…Ø¯Ù…Ø±Ø© (Ø­Ø°Ù Ø¨ÙŠØ§Ù†Ø§Øª Ø¶Ø®Ù…Ø©) Ø¯ÙˆÙ† Ø¥Ø°Ù† ØµØ±ÙŠØ­ Ø¬Ø¯Ø§Ù‹.

Ø§Ø¨Ø¯Ø£ Ø§Ù„Ø¢Ù†. ÙƒÙ† Ù„Ù…Ø§Ø­Ø§Ù‹ØŒ Ø¯Ù‚ÙŠÙ‚Ø§Ù‹ØŒ ÙˆÙ…ÙÙŠØ¯Ø§Ù‹.

## ðŸ“‹ Ù†Ø¸Ù… Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª
- Ø§Ø³ØªØ®Ø¯Ù… Ø¬Ø¯Ø§ÙˆÙ„ Ù„Ø¹Ø±Ø¶ Ø§Ù„Ù‚ÙˆØ§Ø¦Ù…
- Ù„Ø§ ØªØ³ØªØ¹Ù…Ù„ Ø§ÙŠ Ø¥ÙŠÙ…ÙˆØ¬ÙŠ Ù…Ù†Ø§Ø³Ø¨Ø© 
- Ø§Ø¬Ø¹Ù„ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø³Ù‡Ù„Ø© Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© ÙˆØ§Ù„ÙÙ‡Ù…
"""
    
    def conduct_intake(self, chat_history, memory_context="", conversation_context=None):
        """
        Conduct conversational intake with advanced features
        
        Uses enhanced_conduct_intake logic for better performance:
        - Session caching for profile data
        - Multi-step orchestration
        - Intelligent tool selection
        - Context awareness (entity tracking)
        
        Args:
            chat_history: List of conversation messages
            memory_context: Additional context string
            conversation_context: Optional conversation context tracker
        
        Returns:
            Response dictionary with thought, response_text, internal_state, extracted_data
        """
        # Delegate to enhanced version
        return conduct_intake_with_tools(
            self, 
            chat_history, 
            memory_context, 
            conversation_context
        )
    
    def process_user_message(self, message: str, on_thought: Optional[Callable[[str], None]] = None) -> Dict[str, Any]:
        """
        Process user message with dynamic tool calling
        """
        logger.info(f"ðŸ’¬ Processing user message: {message[:50]}...")
        
        # Add user message to memory
        self.add_message("user", message)
        
        # Generate response
        try:
            # We need to pass on_thought to generate_response if we override it, 
            # or handle it here if we copy the logic.
            # Since generate_response is complex, we'll implement a custom loop here or inject it.
            # Best way: Pass it to generate_response if signature allows, or monkey-patch/set instance var temporarily.
            self.current_thought_callback = on_thought
            
            response = self.generate_response()
            
            self.current_thought_callback = None # Cleanup
            
            # Add response to memory
            response_str = str(response)
            self.add_message("assistant", response_str)
            
            return {"response": response_str}
            
        except Exception as e:
            self.current_thought_callback = None
            logger.error(f"âŒ Error processing message: {e}")
            error_msg = f"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£: {str(e)}"
            self.add_message("assistant", error_msg)
            return {"response": error_msg}



    
    
    def think_deeply(
        self,
        query: str,
        context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """
        Deep thinking method with full capabilities
        
        Process:
        1. Analyze query complexity
        2. Decide retrieval strategy (self-regulated)
        3. Retrieve relevant memories
        4. Apply hybrid reasoning
        5. Generate response with confidence
        6. Store in memory
        7. Auto-consolidate if needed
        
        Args:
            query: User query
            context: Additional context
        
        Returns:
            Complete response with reasoning trace
        """
        context = context or {}
        self.current_session["interactions"] += 1
        
        logger.info("="  * 60)
        logger.info(f"ðŸ§  DEEP THINKING - Query: {query[:80]}...")
        logger.info("=" * 60)
        
        # Step 1: Analyze complexity
        complexity, complexity_conf = self.reasoning_engine.analyze_complexity(query, context)
        logger.info(f"ðŸ“Š Complexity: {complexity.value} (confidence: {complexity_conf:.2%})")
        
        # Step 2: Self-regulated retrieval
        retrieved_items = []
        if self.retrieval_system:
            logger.info("âš¡ Self-regulated retrieval...")
            
            retrieval_ctx = RetrievalContext(
                query=query,
                confidence_threshold=complexity_conf,
                query_complexity=complexity.value,
                current_context_size=len(context)
            )
            
            # Use slow thinking for complex queries
            thinking_speed = ThinkingSpeed.SLOW if complexity in [QueryComplexity.COMPLEX, QueryComplexity.EXPERT] else ThinkingSpeed.FAST
            
            retrieval_result = self.retrieval_system.retrieve(
                query=query,
                context=retrieval_ctx,
                thinking_speed=thinking_speed
            )
            
            retrieved_items = retrieval_result.items_retrieved
            logger.info(f"   Retrieved {len(retrieved_items)} memory items")
            logger.info(f"   Decision: {retrieval_result.decision.value}")
            logger.info(f"   Confidence: {retrieval_result.confidence:.2%}")
        
        # Step 3: Apply hybrid reasoning
        logger.info("ðŸ”€ Hybrid reasoning...")
        
        # Determine reasoning mode
        if self.config["reasoning_mode"] == "auto":
            # Auto-select based on complexity
            if complexity == QueryComplexity.SIMPLE:
                mode = ReasoningMode.CHAIN_OF_THOUGHT
            elif complexity == QueryComplexity.MODERATE:
                mode = ReasoningMode.HYBRID
            else:
                mode = ReasoningMode.REACT
        else:
            mode = ReasoningMode(self.config["reasoning_mode"])
        
        # Prepare context with retrieved memories
        enriched_context = context.copy()
        if retrieved_items:
            enriched_context["memories"] = [
                {"content": item.content, "confidence": item.confidence}
                for item in retrieved_items
            ]
        
        # Reason
        reasoning_result = self.reasoning_engine.reason(
            query=query,
            context=enriched_context,
            mode=mode
        )
        
        logger.info(f"   Mode used: {reasoning_result.mode_used.value}")
        logger.info(f"   Steps taken: {len(reasoning_result.steps)}")
        logger.info(f"   Confidence: {reasoning_result.confidence:.2%}")
        
        # Step 4: Store in memory
        if self.memory_system:
            logger.info("ðŸ’¾ Storing in memory...")
            
            # Store query and response
            self.memory_system.remember(
                content=f"Q: {query}  A: {reasoning_result.conclusion}",
                importance=self._determine_importance(complexity),
                tags=self._extract_semantic_tags(query),
                confidence=reasoning_result.confidence,
                source="reasoning_engine"
            )
            
            # Store reasoning trace for future reference
            self.memory_system.remember(
                content=f"Reasoning trace: {reasoning_result.steps[0].thought if reasoning_result.steps else ''}",
                importance=MemoryImportance.MEDIUM,
                tags=["reasoning_trace", complexity.value],
                confidence=reasoning_result.confidence,
                source="thinking_process"
            )
        
        # Step 5: Auto-consolidate if needed
        if self.memory_consolidator and self.current_session["interactions"] % 10 == 0:
            logger.info("ðŸ”„ Auto-consolidating memory...")
            consolidation_stats = self.memory_consolidator.auto_consolidate()
            self.current_session["memory_consolidations"] += 1
            logger.info(f"   Consolidation complete: {consolidation_stats}")
        
        # Step 6: Compile final response
        final_response = {
            "answer": reasoning_result.conclusion,
            "confidence": reasoning_result.confidence,
            "complexity": complexity.value,
            "reasoning_mode": reasoning_result.mode_used.value,
            "reasoning_steps": [step.to_dict() for step in reasoning_result.steps],
            "retrieved_memories_count": len(retrieved_items),
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "session_interaction": self.current_session["interactions"],
                "memory_system": self.config["use_memory"],
                **reasoning_result.metadata
            }
        }
        
        logger.info("=" * 60)
        logger.info(f"âœ… DEEP THINKING COMPLETE - Confidence: {reasoning_result.confidence:.2%}")
        logger.info("=" * 60)
        
        return final_response
    
    def get_reasoning_trace(self, response: Dict[str, Any]) -> str:
        """
        Get human-readable reasoning trace
        
        Args:
            response: Response from think_deeply()
        
        Returns:
            Formatted reasoning trace
        """
        if not response.get("reasoning_steps"):
            return "No reasoning trace Ø§available"
        
        trace = f"""
## ðŸ§  Ù…Ø³Ø§Ø± Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ø¹Ù…ÙŠÙ‚

**Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ:** {response['confidence']:.1%}
**Ø§Ù„ØªØ¹Ù‚ÙŠØ¯:** {response['complexity']}
**Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªÙÙƒÙŠØ±:** {response['reasoning_mode']}
**Ø¹Ø¯Ø¯ Ø§Ù„Ø°ÙƒØ±ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©:** {response['retrieved_memories_count']}

---

## Ø§Ù„Ø®Ø·ÙˆØ§Øª:

"""
        
        for step in response.get("reasoning_steps", []):
            trace += f"""
### Ø§Ù„Ø®Ø·ÙˆØ© {step['step']}: {step['mode']}
**Ø§Ù„ÙÙƒØ±Ø©:** {step['thought']}
"""
            if step.get('action'):
                trace += f"**Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡:** {step['action']}\n"
            if step.get('observation'):
                trace += f"**Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø©:** {step['observation']}\n"
            
            trace += f"**Ø§Ù„Ø«Ù‚Ø©:** {step['confidence']:.1%}\n"
        
        trace += f"""
---

## Ø§Ù„Ø®Ù„Ø§ØµØ©
{response['answer']}
"""
        
        return trace
    
    def get_memory_summary(self) -> Dict[str, Any]:
        """Get summary of memory system state"""
        if not self.memory_system:
            return {"status": "Memory system not enabled"}
        
        summary = self.memory_system.get_memory_summary()
        
        if self.memory_consolidator:
            summary["consolidations"] = {
                "total": self.current_session["memory_consolidations"],
                "history": self.memory_consolidator.get_consolidation_history(limit=5)
            }
        
        return summary
    
    def get_session_stats(self) -> Dict[str, Any]:
        """Get current session statistics"""
        stats = {
            **self.current_session,
            "duration_minutes": (datetime.now() - self.current_session["start_time"]).total_seconds() / 60
        }
        
        if self.retrieval_system:
            stats["retrieval_stats"] = self.retrieval_system.get_stats()
        
        if self.memory_system:
            stats["memory_summary"] = self.get_memory_summary()
        
        return stats
    
    # ===== Case Planning & Execution (Merged from CasePlanner + ExecutorAgent) =====
    
    def create_and_execute_plan(
        self,
        case_facts: str,
        initial_analysis: Dict[str, Any] = None,
        case_type: str = None,
        case_id: str = None
    ) -> Dict[str, Any]:
        """
        Combined method: Create plan AND execute it with tracking
        Ø¯Ù…Ø¬ Ù…Ù† CasePlanner Ùˆ ExecutorAgent Ù…Ø¹ ØªØªØ¨Ø¹ Ø§Ù„Ø®Ø·ÙˆØ§Øª
        
        Args:
            case_facts: Case facts
            initial_analysis: Initial analysis results
            case_type: Type of case
            case_id: Case ID
            
        Returns:
            Complete execution results with plan and reports
        """
        logger.info("ðŸ“‹ Creating and executing case plan with tracking...")
        
        # Step 1: Create Plan
        plan = self.create_plan(case_facts, initial_analysis, case_type)
        
        # Step 2: Initialize Plan Tracker
        plan_id = case_id or f"plan_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø®Ø·Ø© Ø¥Ù„Ù‰ Ø®Ø·ÙˆØ§Øª Ù„Ù„Ù€ tracker
        tracker_steps = []
        for area in plan.get('analysis_areas', []):
            tracker_steps.append({
                "title": area.get('area'),
                "description": area.get('description', '')
            })
        
        # Ø¥Ø¶Ø§ÙØ© Ø®Ø·ÙˆØ© Ù†Ù‡Ø§Ø¦ÙŠØ© Ù„Ù„ØªÙˆØµÙŠØ©
        tracker_steps.append({
            "title": "Ø§Ù„ØªÙˆØµÙŠØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©",
            "description": "ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ØªØ­Ø§Ù„ÙŠÙ„ ÙˆØ¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªÙˆØµÙŠØ© Ø§Ù„Ø´Ø§Ù…Ù„Ø©"
        })
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø·Ø© ÙÙŠ Ø§Ù„Ù€ tracker
        plan_json = self.plan_tracker.create_plan(
            plan_id=plan_id,
            title=f"Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚Ø¶ÙŠØ©: {case_type or 'Ø¹Ø§Ù…'}",
            description=case_facts[:200] + "..." if len(case_facts) > 200 else case_facts,
            steps=tracker_steps
        )
        
        logger.info(f"ðŸ“Š Plan Tracker initialized with {len(tracker_steps)} steps")
        
        # Step 3: Execute Plan with tracking
        case_data = {
            "case_id": case_id,
            "facts": case_facts,
            "general_agent_analysis": initial_analysis,
            "suggested_case_type": case_type,
            "plan": plan
        }
        
        execution_results = self.execute_plan_with_tracking(case_data, plan)
        
        # Mark plan as completed
        final_plan_json = self.plan_tracker.mark_plan_completed()
        
        return {
            "plan": plan,
            "plan_tracker_json": final_plan_json,
            **execution_results
        }
    
    def execute_plan_with_tracking(
        self,
        case_data: Dict[str, Any],
        plan: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Execute plan with step-by-step tracking and streaming
        ØªÙ†ÙÙŠØ° Ø§Ù„Ø®Ø·Ø© Ù…Ø¹ ØªØªØ¨Ø¹ ÙƒÙ„ Ø®Ø·ÙˆØ© ÙˆØ¨Ø« Ø§Ù„Ø£Ø­Ø¯Ø§Ø«
        """
        logger.info("ðŸš€ Executing plan with tracking...")
        
        # Initialize streaming
        from ..streaming.events import EventType, StepStatus
        from ..streaming.manager import stream_manager
        
        # Use plan_tracker ID or case_id
        plan_id = self.plan_tracker.current_plan.plan_id if self.plan_tracker.current_plan else case_data.get("case_id", "unknown")
        
        # Register streamer
        streamer = stream_manager.register(plan_id)
        
        # Notify plan started
        streamer.push_plan_event(
            EventType.PLAN_CREATED, 
            {"title": plan.get("title", "Processing Case"), "steps_count": len(plan.get("analysis_areas", []))}
        )
        
        case_id = case_data.get("case_id")
        case_facts = case_data.get("facts")
        
        analysis_results = []
        analysis_areas = plan.get("analysis_areas", [])
        total_steps = len(analysis_areas) + 1  # +1 for final recommendation
        
        step_num = 1
        for area_spec in analysis_areas:
            area_name = area_spec.get("area")
            description = area_spec.get("description", "")
            
            # Start step in tracker
            self.plan_tracker.start_step(step_num)
            logger.info(f"â–¶ï¸ [{step_num}/{total_steps}] Analyzing: {area_name}")
            
            # Stream: Step Started
            streamer.push_step_event(
                step_id=step_num,
                status=StepStatus.IN_PROGRESS,
                message=f"Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„: {area_name}",
                progress=int((step_num / total_steps) * 100)
            )
            
            try:
                # Use deep thinking for this area
                query = (
                    f"Ø¨ØµÙØªÙƒ Ù…Ø­Ø§Ù…Ù expert ÙÙŠ {area_name}, Ù‚Ù… Ø¨Ø§Ù„ØªØ§Ù„ÙŠ:\n\n"
                    f"**Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹:**\n{case_facts}\n\n"
                    f"**Ø§Ù„ØªØ±ÙƒÙŠØ² Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:**\n{description}\n\n"
                    f"Ù‚Ø¯Ù… ØªØ­Ù„ÙŠÙ„Ø§Ù‹ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ§Ù‹ Ù…ØªØ®ØµØµØ§Ù‹."
                )
                
                result = self.think_deeply(query, context={"case_id": case_id, "area": area_name})
                
                analysis_results.append({
                    "area": area_name,
                    "analysis": result.get("answer"),
                    "confidence": result.get("confidence"),
                    "complexity": result.get("complexity")
                })
                
                # Complete step in tracker
                self.plan_tracker.complete_step(
                    step_num,
                    f"Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ - Ø§Ù„Ø«Ù‚Ø©: {result.get('confidence', 0):.1%}"
                )
                logger.info(f"âœ… [{step_num}] {area_name} completed")
                
                # Stream: Step Completed
                streamer.push_step_event(
                    step_id=step_num,
                    status=StepStatus.COMPLETED,
                    message=f"ØªÙ… ØªØ­Ù„ÙŠÙ„ {area_name} Ø¨Ù†Ø¬Ø§Ø­",
                    progress=int((step_num / total_steps) * 100)
                )
                
            except Exception as e:
                logger.error(f"âŒ Error in step {step_num}: {e}")
                self.plan_tracker.fail_step(step_num, str(e))
                
                # Stream: Step Failed
                streamer.push_step_event(
                    step_id=step_num,
                    status=StepStatus.FAILED,
                    message=f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ {area_name}: {str(e)}",
                    error=str(e)
                )
            
            step_num += 1
        
        # Final recommendation step
        self.plan_tracker.start_step(step_num)
        logger.info(f"â–¶ï¸ [{step_num}] Compiling final recommendation...")
        
        # Stream: Final Step Started
        streamer.push_step_event(
            step_id=step_num,
            status=StepStatus.IN_PROGRESS,
            message="Ø¬Ø§Ø±ÙŠ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªÙˆØµÙŠØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©...",
            progress=95
        )
        
        try:
            final_recommendation = self._compile_final_recommendation_simple(
                case_facts=case_facts,
                analysis_results=analysis_results
            )
            
            self.plan_tracker.complete_step(step_num, "ØªÙ… Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªÙˆØµÙŠØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©")
            logger.info(f"âœ… [{step_num}] Final recommendation completed")
            
            # Stream: Final Step Completed
            streamer.push_step_event(
                step_id=step_num,
                status=StepStatus.COMPLETED,
                message="ØªÙ… Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªÙˆØµÙŠØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©",
                progress=100
            )
            
            # Stream: Plan Completed
            streamer.push_plan_event(
                EventType.PLAN_COMPLETED,
                {"result": final_recommendation}
            )
            
        except Exception as e:
            logger.error(f"âŒ Error in final recommendation: {e}")
            self.plan_tracker.fail_step(step_num, str(e))
            final_recommendation = {"error": str(e)}
            
            # Stream: Plan Failed
            streamer.push_plan_event(
                EventType.PLAN_FAILED,
                {"error": str(e)}
            )
        
        return {
            "analysis_results": analysis_results,
            "final_recommendation": final_recommendation,
            "completed_at": datetime.now().isoformat()
        }

    def create_plan(
        self,
        case_facts: str,
        initial_analysis: Dict[str, Any] = None,
        case_type: str = None
    ) -> Dict[str, Any]:
        """
        Create execution plan (from CasePlanner)
        
        Instead of creating separate specialist agents we use
        the main agent capabilities with different focus areas
        """
        from ..config.settings import AgentTypes
        
        logger.info("ðŸ“ Creating execution plan...")
        
        # Simplified planning - no separate agents needed
        # The EnhancedGeneralLawyerAgent handles everything
        
        analysis_text = json.dumps(initial_analysis, ensure_ascii=False, indent=2) if initial_analysis else "Ù„Ø§ ÙŠÙˆØ¬Ø¯"
        
        planning_prompt = (
            f"Ø¨ØµÙØªÙƒ Ù…Ø­Ø§Ù…Ù expert, Ø¶Ø¹ Ø®Ø·Ø© ØªØ­Ù„ÙŠÙ„ Ù„Ù„Ù‚Ø¶ÙŠØ© Ø§Ù„ØªØ§Ù„ÙŠØ©.\n\n"
            f"**Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹:**\n{case_facts}\n\n"
            f"**Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£ÙˆÙ„ÙŠ:**\n{analysis_text}\n\n"
            f"**Ù†ÙˆØ¹ Ø§Ù„Ù‚Ø¶ÙŠØ©:** {case_type or 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'}\n\n"
            f"---\n\n"
            f"Ù‚Ø¯Ù… Ø®Ø·Ø© ØªØ­Ù„ÙŠÙ„ Ù…ØªÙƒØ§Ù…Ù„Ø© ØªØ´Ù…Ù„ Ø§Ù„Ù…Ø¬Ø§Ù„Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© (JSON):\n"
            f'{{"analysis_areas": [\n'
            f'  {{"area": "Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ", "priority": 1, "description": "..."}},\n'
            f'  {{"area": "ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ù„Ø©", "priority": 2, "description": "..."}}\n'
            f'],\n'
            f'"execution_strategy": "sequential",\n'
            f'"estimated_complexity": "simple/medium/complex",\n'
            f'"key_points": ["Ù†Ù‚Ø·Ø© 1", "Ù†Ù‚Ø·Ø© 2"]\n'
            f'}}\n\n'
            f"Ø£Ø¹Ø¯ JSON ÙÙ‚Ø·:"
        )
        
        try:
            self.add_message("user", planning_prompt)
            plan_response = self.generate_response()
            self.add_message("assistant", plan_response)
            
            # Parse JSON
            import json
            if "{" in plan_response:
                start = plan_response.find("{")
                end = plan_response.rfind("}") + 1
                json_str = plan_response[start:end]
                plan = json.loads(json_str)
            else:
                plan = self._create_default_plan_simple(case_type)
                
        except Exception as e:
            logger.warning(f"âš ï¸ Plan creation failed: {e}, using default")
            plan = self._create_default_plan_simple(case_type)
        
        logger.info(f"âœ… Plan created with {len(plan.get('analysis_areas', []))} areas")
        return plan
    
    def _create_default_plan_simple(self, case_type: str = None) -> Dict[str, Any]:
        """Create default plan when LLM fails"""
        return {
            "analysis_areas": [
                {
                    "area": "Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ø¹Ø§Ù…",
                    "priority": 1,
                    "description": "ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„Ù‚Ø¶ÙŠØ©"
                }
            ],
            "execution_strategy": "sequential",
            "estimated_complexity": "medium",
            "key_points": ["ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹", "Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ"]
        }
    
    def execute_plan_simple(
        self,
        case_data: Dict[str, Any],
        plan: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Execute the legal plan (simplified - without dynamic agents)
        
        Instead of creating separate specialist agents we use
        the general agent deep thinking for each area
        """
        logger.info("ðŸš€ Executing legal plan (simplified mode)...")
        
        case_id = case_data.get("case_id")
        case_facts = case_data.get("facts")
        
        analysis_results = []
        analysis_areas = plan.get("analysis_areas", [])
        
        for area_spec in analysis_areas:
            area_name = area_spec.get("area")
            description = area_spec.get("description", "")
            
            logger.info(f"â–¶ï¸ Analyzing: {area_name}")
            
            # Use deep thinking for this area
            query = (
                f"Ø¨ØµÙØªÙƒ Ù…Ø­Ø§Ù…Ù expert ÙÙŠ {area_name}, Ù‚Ù… Ø¨Ø§Ù„ØªØ§Ù„ÙŠ:\n\n"
                f"**Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹:**\n{case_facts}\n\n"
                f"**Ø§Ù„ØªØ±ÙƒÙŠØ² Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:**\n{description}\n\n"
                f"Ù‚Ø¯Ù… ØªØ­Ù„ÙŠÙ„Ø§Ù‹ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ§Ù‹ Ù…ØªØ®ØµØµØ§Ù‹."
            )
            
            result = self.think_deeply(query, context={"case_id": case_id, "area": area_name})
            
            analysis_results.append({
                "area": area_name,
                "analysis": result.get("answer"),
                "confidence": result.get("confidence"),
                "complexity": result.get("complexity")
            })
            
            logger.info(f"âœ… {area_name} completed (confidence: {result.get('confidence', 0):.1%})")
        
        # Compile final recommendation
        final_recommendation = self._compile_final_recommendation_simple(
            case_facts=case_facts,
            analysis_results=analysis_results
        )
        
        return {
            "analysis_results": analysis_results,
            "final_recommendation": final_recommendation,
            "completed_at": datetime.now().isoformat()
        }
    
    def _compile_final_recommendation_simple(
        self,
        case_facts: str,
        analysis_results: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Compile final recommendation from all analyses"""
        logger.info("ðŸ“Š Compiling final recommendation...")
        
        
        all_analyses = "\n\n".join([
            f"## {res['area']}\n{res['analysis']}\n(Ø«Ù‚Ø©: {res['confidence']:.1%})"
            for res in analysis_results
        ])
        
        compilation_prompt = (
            f"Ø¨ØµÙØªÙƒ Ø§Ù„Ù…Ø­Ø§Ù…ÙŠ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ, Ø§Ø¬Ù…Ø¹ Ø§Ù„ØªØ­Ø§Ù„ÙŠÙ„ Ø§Ù„ØªØ§Ù„ÙŠØ© ÙÙŠ ØªÙˆØµÙŠØ© Ù†Ù‡Ø§Ø¦ÙŠØ© Ù…ØªÙƒØ§Ù…Ù„Ø©:\n\n"
            f"**Ø§Ù„ÙˆÙ‚Ø§Ø¦Ø¹:**\n{case_facts}\n\n"
            f"**Ø§Ù„ØªØ­Ø§Ù„ÙŠÙ„ Ø§Ù„Ù…ØªØ®ØµØµØ©:**\n{all_analyses}\n\n"
            f"---\n\n"
            f"Ù‚Ø¯Ù… ØªÙˆØµÙŠØ© Ù†Ù‡Ø§Ø¦ÙŠØ© Ø´Ø§Ù…Ù„Ø© ØªØªØ¶Ù…Ù†:\n"
            f"1. Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠ\n"
            f"2. Ø§Ù„Ø±Ø£ÙŠ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ\n"
            f"3. Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©\n"
            f"4. Ø§Ù„ØªÙˆØµÙŠØ§Øª"
        )
        
        self.add_message("user", compilation_prompt)
        final_response = self.generate_response()
        self.add_message("assistant", final_response)
        
        return {
            "recommendation_text": final_response,
            "compiled_at": datetime.now().isoformat(),
            "based_on_analyses": len(analysis_results),
            "average_confidence": sum(r["confidence"] for r in analysis_results) / len(analysis_results) if analysis_results else 0
        }
    
    # ===== Helper Methods =====
    
    def _determine_importance(self, complexity: QueryComplexity) -> MemoryImportance:
        """Determine memory importance based on query complexity"""
        if complexity == QueryComplexity.EXPERT:
            return MemoryImportance.CRITICAL
        elif complexity == QueryComplexity.COMPLEX:
            return MemoryImportance.HIGH
        elif complexity == QueryComplexity.MODERATE:
            return MemoryImportance.MEDIUM
        else:
            return MemoryImportance.LOW
    
    def _extract_semantic_tags(self, text: str) -> List[str]:
        """Extract semantic tags from text"""
        # Simple keyword extraction (can be enhanced)
        keywords = []
        
        # Legal domain keywords
        legal_terms = ["Ø¹Ù‚Ø¯", "Ù‚Ø§Ù†ÙˆÙ†", "Ù…Ø­ÙƒÙ…Ø©", "Ù‚Ø¶ÙŠØ©", "Ø¯Ø¹ÙˆÙ‰", "Ø­ÙƒÙ…", "Ù†Ø¸Ø§Ù…"]
        
        for term in legal_terms:
            if term in text:
                keywords.append(term)
        
        return keywords[:5]  # Limit to 5 tags


__all__ = ["EnhancedGeneralLawyerAgent"]
